
# A complete list of papers about adversarial examples

It appears that the [List of All Adversarial Example Papers](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html) has been experiencing crashes over the past few days. In the absence of this valuable resource, staying up-to-date with the latest research papers in this field has become challenging. Consequently, I created a repository aimed at aggregating and maintaining the most current papers in this domain. While this repository may not encompass every paper, I did try. If you find any papers we have missed, just drop me an [email](mailto:xswanghuster@gmail.com). We have included the [data](./nicholas.md) from [List of All Adversarial Example Papers](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html) till 2023-09-01.
## 2023-11-21
+ [ Boost Adversarial Transferability by Uniform Scale and Mix Mask Method](https://arxiv.org//abs/2311.12051)`uncheck`

	Tao Wang, Zijian Ying, Qianmu Li, zhichao Lian


+ [ BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive  Learning](https://arxiv.org//abs/2311.12075)`uncheck`

	Siyuan Liang, Mingli Zhu, Aishan Liu, Baoyuan Wu, Xiaochun Cao, Ee-Chien Chang


+ [ Investigating Weight-Perturbed Deep Neural Networks With Application in  Iris Presentation Attack Detection](https://arxiv.org//abs/2311.12764)`uncheck`

	Renu Sharma, Redwan Sony, Arun Ross


+ [ Iris Presentation Attack: Assessing the Impact of Combining Vanadium  Dioxide Films with Artificial Eyes](https://arxiv.org//abs/2311.12773)`uncheck`

	Darshika Jauhari, Renu Sharma, Cunjian Chen, Nelson Sepulveda, Arun Ross


+ [ ODDR: Outlier Detection & Dimension Reduction Based Defense Against  Adversarial Patches](https://arxiv.org//abs/2311.12084)`uncheck`

	Nandish Chattopadhyay, Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammad Shafique


+ [ Attacking Motion Planners Using Adversarial Perception Errors](https://arxiv.org//abs/2311.12722)`uncheck`

	Jonathan Sadeghi, Nicholas A. Lord, John Redford, Romain Mueller


+ [ Masked Autoencoders Are Robust Neural Architecture Search Learners](https://arxiv.org//abs/2311.12086)`uncheck`

	Yiming Hu, Xiangxiang Chu, Bo Zhang


+ [ Adversarial Reweighting Guided by Wasserstein Distance for Bias  Mitigation](https://arxiv.org//abs/2311.12684)`uncheck`

	Xuan Zhao, Simone Fabbrizzi, Paula Reyero Lobo, Siamak Ghodsi, Klaus Broelemann, Steffen Staab, Gjergji Kasneci


+ [ Attacks of fairness in Federated Learning](https://arxiv.org//abs/2311.12715)`uncheck`

	Joseph Rance, Filip Svoboda


+ [ DefensiveDR: Defending against Adversarial Patches using Dimensionality  Reduction](https://arxiv.org//abs/2311.12211)`uncheck`

	Nandish Chattopadhyay, Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammad Shafique


## 2023-11-20
+ [ Safety-aware Causal Representation for Trustworthy Reinforcement  Learning in Autonomous Driving ](https://arxiv.org//abs/2311.10747)

	Haohong Lin, Wenhao Ding, Zuxin Liu, Yaru Niu, Jiacheng Zhu, Yuming Niu, Ding Zhao


+ [ Assessing Prompt Injection Risks in 200+ Custom GPTs ](https://arxiv.org//abs/2311.11538)

	Jiahao Yu, Yuhang Wu, Dong Shu, Mingyu Jin, Xinyu Xin


+ [ Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI  Systems ](https://arxiv.org//abs/2311.11796)

	Guangjing Wang, Ce Zhou, Yuanda Wang, Bocheng Chen, Hanqing Guo, Qiben Yan


+ [ Generating Valid and Natural Adversarial Examples with Large Language Models ](http://arxiv.org/abs/2311.11861)

	Zimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, Anh Nguyen


+ [ AdvGen: Physical Adversarial Attack on Face Presentation Attack Detection Systems ](http://arxiv.org/abs/2311.11753)

	Sai Amrit Patnaik, Shivali Chansoriya, Anil K. Jain, Anoop M. Namboodiri


+ [ Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems ](http://arxiv.org/abs/2311.11796)

	Guangjing Wang, Ce Zhou, Yuanda Wang, Bocheng Chen, Hanqing Guo, Qiben Yan


+ [ Understanding Variation in Subpopulation Susceptibility to Poisoning Attacks ](http://arxiv.org/abs/2311.11544)

	Evan Rose, Fnu Suya, David Evans


+ [ Training robust and generalizable quantum models ](http://arxiv.org/abs/2311.11871)

	Julian Berberich, Daniel Fink, Daniel Pranjić, Christian Tutschku, Christian Holm       


+ [ BrainWash: A Poisoning Attack to Forget in Continual Learning ](http://arxiv.org/abs/2311.11995)

	Ali Abbasi, Parsa Nooralinejad, Hamed Pirsiavash, Soheil Kolouri


## 2023-11-19
+ [ Robust Network Slicing: Multi-Agent Policies, Adversarial Attacks, and  Defensive Strategies ](https://arxiv.org//abs/2311.11206)

	Feng Wang, M. Cenk Gursoy, Senem Velipasalar


+ [ Adversarial Prompt Tuning for Vision-Language Models ](http://arxiv.org/abs/2311.11261)

	Jiaming Zhang, Xingjun Ma, Xin Wang, Lingyu Qiu, Jiaqi Wang, Yu-Gang Jiang, Jitao Sang  


+ [ TextGuard: Provable Defense against Backdoor Attacks on Text  Classification ](https://arxiv.org//abs/2311.11225)

	Hengzhi Pei, Jinyuan Jia, Wenbo Guo, Bo Li, Dawn Song


## 2023-11-18
+ [ Improving Adversarial Transferability by Stable Diffusion ](http://arxiv.org/abs/2311.11017)

 	Jiayang Liu, Siyu Zhu, Siyuan Liang, Jie Zhang, Han Fang, Weiming Zhang, Ee-Chien Chang 


+ [ Attention-Based Real-Time Defenses for Physical Adversarial Attacks in Vision Applications ](http://arxiv.org/abs/2311.11191)

	Giulio Rossolini, Alessandro Biondi, Giorgio Buttazzo


+ [ PACOL: Poisoning Attacks Against Continual Learners ](https://arxiv.org//abs/2311.10919)

	Huayu Li, Gregory Ditzler


+ [ Robust Network Slicing: Multi-Agent Policies, Adversarial Attacks, and Defensive Strategies ](http://arxiv.org/abs/2311.11206)

 	Feng Wang, M. Cenk Gursoy, Senem Velipasalar


## 2023-11-17
+ [ Robustness Enhancement in Neural Networks with Alpha-Stable Training  Noise ](https://arxiv.org//abs/2311.10803)

	Xueqiong Yuan, Jipeng Li, Ercan Engin Kuruoğlu

	
+ [ Breaking Temporal Consistency: Generating Video Universal Adversarial Perturbations Using Image Models ](http://arxiv.org/abs/2311.10366)

 	Hee-Seon Kim, Minji Son, Minbeom Kim, Myung-Joon Kwon, Changick Kim


+ [ PACOL: Poisoning Attacks Against Continual Learners ](http://arxiv.org/abs/2311.10919)

	Huayu Li, Gregory Ditzler


+ [ Two-Factor Authentication Approach Based on Behavior Patterns for Defeating Puppet Attacks ](http://arxiv.org/abs/2311.10389)

	Wenhao Wang, Guyue Li, Zhiming Chu, Haobo Li, Daniele Faccio


## 2023-11-16
+ [Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting](http://arxiv.org/abs/2311.09790)

    Ilbert Romain, V. Hoang Thai, Zhang Zonghua, Palpanas Themis


+ [ Hijacking Large Language Models via Adversarial In-Context Learning](http://arxiv.org/abs/2311.09948)

    Yao Qiang, Xiangyu Zhou, Dongxiao Zhu


+ [ Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking](http://arxiv.org/abs/2311.09827)

    Nan Xu, Fei Wang, Ben Zhou, Bang Zheng Li, Chaowei Xiao, Muhao Chen


+ [ Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations](http://arxiv.org/abs/2311.09763)

    Wenjie Mo, Jiashu Xu, Qin Liu, Jiongxiao Wang, Jun Yan, Chaowei Xiao, Muhao Chen


+ [ On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models](http://arxiv.org/abs/2311.09641)

    Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, Chaowei Xiao


+ [ Towards more Practical Threat Models in Artificial Intelligence Security](http://arxiv.org/abs/2311.09994)

    Kathrin Grosse, Lukas Bieringer, Tarek Richard Besold, Alexandre Alahi


## 2023-11-15
+ [ Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts](http://arxiv.org/abs/2311.09127)

    Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, Lichao Sun


+ [ Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment](http://arxiv.org/abs/2311.09433)  

    Haoran Wang, Kai Shu


+ [ Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing](http://arxiv.org/abs/2311.09024)

    A K Iowa State University Nirala, A New York University Joshi, C New York University Hegde, S Iowa State University Sarkar


+ [ Adversarially Robust Spiking Neural Networks Through Conversion](http://arxiv.org/abs/2311.09266)

    Ozan Özdenizci, Robert Legenstein


+ [ Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization](http://arxiv.org/abs/2311.09096)

    Zhexin Zhang, Junxiao Yang, Pei Ke, Minlie Huang


+ [ Privacy Threats in Stable Diffusion Models](http://arxiv.org/abs/2311.09355)

    Thomas Cilloni, Charles Fleming, Charles Walter


+ [ How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities](http://arxiv.org/abs/2311.09447)

    Lingbo Mo, Boshi Wang, Muhao Chen, Huan Sun


+ [ MirrorNet: A TEE-Friendly Framework for Secure On-device DNN Inference](http://arxiv.org/abs/2311.09489)

    Ziyu Liu, Yukui Luo, Shijin Duan, Tong Zhou, Xiaolin Xu


+ [ Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models](http://arxiv.org/abs/2311.09428)

    Yueqing Liang, Lu Cheng, Ali Payani, Kai Shu


+ [ JAB: Joint Adversarial Prompting and Belief Augmentation](http://arxiv.org/abs/2311.09473)

    Ninareh Mehrabi, Palash Goyal, Anil Ramakrishna, Jwala Dhamala, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, Rahul Gupta


## 2023-11-14
+ [ Towards Improving Robustness Against Common Corruptions in Object Detectors Using Adversarial Contrastive Learning](http://arxiv.org/abs/2311.07928)

    Shashank Kotyan, Danilo Vasconcellos Vargas


+ [ Physical Adversarial Examples for Multi-Camera Systems](http://arxiv.org/abs/2311.08539)

    Ana Răduţoiu, Jan-Philipp Schulze, Philip Sperl, Konstantin Böttinger


+ [ DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Pre-trained Language Models](http://arxiv.org/abs/2311.08598)

    Yibo Wang, Xiangjue Dong, James Caverlee, Philip S. Yu


+ [ On The Relationship Between Universal Adversarial Attacks And Sparse Representations](http://arxiv.org/abs/2311.08265)

    Dana Weitzner, Raja Giryes


+ [ A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](http://arxiv.org/abs/2311.08268)   

    Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, Shujian Huang


+ [ Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets](http://arxiv.org/abs/2311.08662)

    Vatsal Gupta, Pranshu Pandya, Tushar Kataria, Vivek Gupta, Dan Roth


+ [ The Perception-Robustness Tradeoff in Deterministic Image Restoration](http://arxiv.org/abs/2311.09253)

    Guy Ohayon, Tomer Michaeli, Michael Elad


## 2023-11-13
+ [ Adversarial Purification for Data-Driven Power System Event Classifiers with Diffusion Models](http://arxiv.org/abs/2311.07110)

    Yuanbin Cheng, Koji Yamashita, Jim Follum, Nanpeng Yu


+ [ Parrot-Trained Adversarial Examples: Pushing the Practicality of Black-Box Audio Attacks against Speaker Recognition Models](http://arxiv.org/abs/2311.07780)

    Rui Duan, Zhe Qu, Leah Ding, Yao Liu, Zhuo Lu


+ [ An Extensive Study on Adversarial Attack against Pre-trained Models of Code](http://arxiv.org/abs/2311.07553)

    Xiaohu Du, Ming Wen, Zichao Wei, Shangwen Wang, Hai Jin


+ [ Untargeted Black-box Attacks for Social Recommendations](http://arxiv.org/abs/2311.07127)

    Wenqi Fan, Shijie Wang, Xiao-yong Wei, Xiaowei Mei, Qing Li


+ [ On the Robustness of Neural Collapse and the Neural Collapse of Robustness](http://arxiv.org/abs/2311.07444)

    Jingtong Su, Ya Shi Zhang, Nikolaos Tsilivis, Julia Kempe


+ [ Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data ](http://arxiv.org/abs/2311.07550)

    Bart Pleiter, Behrad Tajalli, Stefanos Koffas, Gorka Abad, Jing Xu, Martha Larson, Stjepan Picek


## 2023-11-12
+ [ Learning Globally Optimized Language Structure via Adversarial Training](http://arxiv.org/abs/2311.06771)

	Xuwang Yin


+ [ Contractive Systems Improve Graph Neural Networks Against Adversarial Attacks](http://arxiv.org/abs/2311.06942)

    Moshe Eliasof, Davide Murari, Ferdia Sherry, Carola-Bibiane Schönlieb


+ [ Analytical Verification of Deep Neural Network Performance for Time-Synchronized Distribution System State Estimation](http://arxiv.org/abs/2311.06973)

        Behrouz Azimian, Shiva Moshtagh, Anamitra Pal, Shanshan Ma


+ [ DialMAT: Dialogue-Enabled Transformer with Moment-Based Adversarial Training](http://arxiv.org/abs/2311.06855)

    Kanta Kaneda, Ryosuke Korekata, Yuiga Wada, Shunya Nagashima, Motonari Kambara, Yui Iioka, Haruka Matsuo, Yuto Imai, Takayuki Nishimura, Komei Sugiura


## 2023-11-10
+ [ Robust Adversarial Attacks Detection for Deep Learning based Relative  Pose Estimation for Space Rendezvous](https://arxiv.org//abs/2311.05992)

	Ziwei Wang, Nabil Aouf, Jose Pizarro, Christophe Honvault


+ [ Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the  Wild](https://arxiv.org//abs/2311.06237)

	Nanna Inie, Jonathan Stray, Leon Derczynski


+ [ Scale-MIA: A Scalable Model Inversion Attack against Secure Federated  Learning via Latent Space Reconstruction](https://arxiv.org//abs/2311.05808)

	Shanghao Shi, Ning Wang, Yang Xiao, Chaoyu Zhang, Yi Shi, Y.Thomas Hou, Wenjing Lou


+ [ Does Differential Privacy Prevent Backdoor Attacks in Practice?](https://arxiv.org//abs/2311.06227)

	Fereshteh Razmi, Jian Lou, Li Xiong


+ [Flatness-aware Adversarial Attack](https://arxiv.org/abs/2311.06423)

	Mingyuan Fan, Xiaodan Li, Cen Chen, Yinggui Wang


+ [Robust Adversarial Attacks Detection for Deep Learning based Relative Pose Estimation for Space Rendezvous.](https://arxiv.org/abs/2311.05992)

	Ziwei Wang, Nabil Aouf, Jose Pizarro, Christophe Honvault


+ [Fight Fire with Fire: Combating Adversarial Patch Attacks using Pattern-randomized Defensive Patches](https://arxiv.org/abs/2311.06122)
	
	Jianan Feng, Jiachun Li, Changqing Miao, Jianjun Huang, Wei You, Wenchang Shi, Bin Liang


+ [Resilient and constrained consensus against adversarial attacks: A distributed MPC framework](https://arxiv.org/abs/2311.05935)

	Henglai Wei, Kunwu Zhang, Hui Zhang, Yang Shi


+ [CALLOC: Curriculum Adversarial Learning for Secure and Robust Indoor Localization](https://arxiv.org/abs/2311.06361)

	Danish Gufran, Sudeep Pasricha

+ [Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration](https://arxiv.org/abs/2311.06062)

	Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang


## 2023-11-09
+ [ Counter-Empirical Attacking based on Adversarial Reinforcement Learning  for Time-Relevant Scoring System](https://arxiv.org//abs/2311.05144)

	Xiangguo Sun, Hong Cheng, Hang Dong, Bo Qiao, Si Qin, Qingwei Lin


+ [ SynFacePAD 2023: Competition on Face Presentation Attack Detection Based  on Privacy-aware Synthetic Training Data](https://arxiv.org//abs/2311.05336)

	Meiling Fang, Marco Huber, Julian Fierrez, Raghavendra Ramachandra, Naser Damer, Alhasan Alkhaddour, Maksim Kasantcev, Vasiliy Pryadchenko, Ziyuan Yang, Huijie Huangfu, Yingyu Chen, Yi Zhang, Yuchen Pan, Junjun Jiang, Xianming Liu, Xianyun Sun, Caiyong Wang, Xingyu Liu, Zhaohua Chang, Guangzhe Zhao, Juan Tapia, Lazaro Gonzalez-Soler, Carlos Aravena, Daniel Schulz


+ [ Counter-Empirical Attacking based on Adversarial Reinforcement Learning  for Time-Relevant Scoring System](https://arxiv.org//abs/2311.05144)

	Xiangguo Sun, Hong Cheng, Hang Dong, Bo Qiao, Si Qin, Qingwei Lin


+ [ SCAAT: Improving Neural Network Interpretability via Saliency  Constrained Adaptive Adversarial Training](https://arxiv.org//abs/2311.05143)

	Rui Xu, Wenkang Qin, Peixiang Huang, Haowang, Lin Luo


+ [ Training Robust Deep Physiological Measurement Models with Synthetic  Video-based Data](https://arxiv.org//abs/2311.05371)

	Yuxuan Ou, Yuzhe Zhang, Yuntang Wang, Shwetak Patel, Daniel McDuf, Xin Liu

+ [ FigStep: Jailbreaking Large Vision-language Models via Typographic  Visual Prompts](https://arxiv.org//abs/2311.05608)

	Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, Xiaoyun Wang


## 2023-11-8
+ [ Familiarity-Based Open-Set Recognition Under Adversarial Attacks](https://arxiv.org//abs/2311.05006)

	Philip Enevoldsen, Christian Gundersen, Nico Lang, Serge Belongie, Christian Igel


+ [ Edge-assisted U-Shaped Split Federated Learning with Privacy-preserving  for Internet of Things](https://arxiv.org//abs/2311.04944)

	Hengliang Tang, Zihang Zhao, Detian Liu, Yang Cao, Shiqiang Zhang, Siqing You


+ [ DEMASQ: Unmasking the ChatGPT Wordsmith](https://arxiv.org//abs/2311.05019)

	Kavita Kumari, Alessandro Pegoraro, Hossein Fereidooni, Ahmad-Reza Sadeghi


+ [ On the steerability of large language models toward data-driven personas](https://arxiv.org//abs/2311.04978)

	Junyi Li, Ninareh Mehrabi, Charith Peris, Palash Goyal, Kai-Wei Chang, Aram Galstyan, Richard Zemel, Rahul Gupta


## 2023-11-7
+ [ FD-MIA: Efficient Attacks on Fairness-enhanced Models](https://arxiv.org/abs/2311.03865)

	Huan Tian, Guangsheng Zhang, Bo Liu, Tianqing Zhu, Ming Ding, Wanlei Zhou


+ [ Unveiling Safety Vulnerabilities of Large Language Models](https://arxiv.org/abs/2311.04124)
	
	George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret Anaby-Tavor, Orna Raz, Eitan Farchi


## 2023-11-6
+ [ A Preference Learning Approach to Develop Safe and Personalizable  Autonomous Vehicles](https://arxiv.org//abs/2311.02099)

	Ruya Karagulle, Nikos Arechiga, Andrew Best, Jonathan DeCastro, Necmiye Ozay


+ [ Making Harmful Behaviors Unlearnable for Large Language Models](https://arxiv.org//abs/2311.02105)

	Xin Zhou, Yi Lu, Ruotian Ma, Tao Gui, Qi Zhang, Xuanjing Huang


+ [ Uncertainty Quantification of Deep Learning for Spatiotemporal Data:  Challenges and Opportunities](https://arxiv.org//abs/2311.02485)

	Wenchong He, Zhe Jiang


+ [ On the Intersection of Self-Correction and Trust in Language Models](https://arxiv.org//abs/2311.02801)

	Satyapriya Krishna


+ [ Preserving Privacy in GANs Against Membership Inference Attack](https://arxiv.org//abs/2311.03172)

	Mohammadhadi Shateri, Francisco Messina, Fabrice Labeau, Pablo Piantanida


+ [ DeepInception: Hypnotize Large Language Model to Be Jailbreaker](https://arxiv.org//abs/2311.03191)

	Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, Bo Han


## 2023-11-5
+ [ Pilot-Based Key Distribution and Encryption for Secure Coherent Passive  Optical Networks](https://arxiv.org//abs/2311.02554)

	Haide Wang, Ji Zhou, Qingxin Lu, Jianrui Zeng, Yongqing Liao, Weiping Liu, Changyuan Yu, Zhaohui Li


+ [ ELEGANT: Certified Defense on the Fairness of Graph Neural Networks](https://arxiv.org/abs/2311.02757)

	Yushun Dong; Binchi Zhang; Hanghang Tong; Jundong Li


## 2023-11-4
+ [ From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects  in Diffusion Models](https://arxiv.org//abs/2311.02373)

	Zhuoshi Pan, Yuguang Yao, Gaowen Liu, Bingquan Shen, H. Vicky Zhao, Ramana Rao Kompella, Sijia Liu


## 2023-11-3
+ [ Can AI Mitigate Human Perceptual Biases? A Pilot Study](https://arxiv.org//abs/2311.00706)

	Ross Geuy, Nate Rising, Tiancheng Shi, Meng Ling, Jian Chen


+ [ SCPO: Safe Reinforcement Learning with Safety Critic Policy Optimization](https://arxiv.org//abs/2311.00880)

	Jaafar Mhamed, Shangding Gu


+ [ Robust Identity Perceptual Watermark Against Deepfake Face Swapping](https://arxiv.org//abs/2311.01357)

	Tianyi Wang, Mengxiao Huang, Harry Cheng, Bin Ma, Yinglong Wang


+ [ A Call to Arms: AI Should be Critical for Social Media Analysis of  Conflict Zones](https://arxiv.org//abs/2311.00810)

	Afia Abedin, Abdul Bais, Cody Buntain, Laura Courchesne, Brian McQuinn, Matthew E. Taylor, Muhib Ullah


+ [ Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game](https://arxiv.org//abs/2311.01011)

	Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, Stuart Russell


+ [ MIST: Defending Against Membership Inference Attacks Through  Membership-Invariant Subspace Training](https://arxiv.org//abs/2311.00919)

	Jiacheng Li, Ninghui Li, Bruno Ribeiro


+ [ Reputation Systems for Supply Chains: The Challenge of Achieving Privacy  Preservation](https://arxiv.org//abs/2311.01060)

	Lennart Bader, Jan Pennekamp, Emildeon Thevaraj, Maria Spiß, Salil S. Kanhere, Klaus Wehrle


## 2023-11-2
+ [ Optimal Cost Constrained Adversarial Attacks For Multiple Agent Systems](https://arxiv.org//abs/2311.00859)

	Ziqing Lu, Guanlin Liu, Lifeng Cai, Weiyu Xu


+ [ SCPO: Safe Reinforcement Learning with Safety Critic Policy Optimization](https://arxiv.org//abs/2311.00880)

	Jaafar Mhamed, Shangding Gu


+ [ Generate and Pray: Using SALLMS to Evaluate the Security of LLM  Generated Code](https://arxiv.org//abs/2311.00889)

	Mohammed Latif Siddiq, Joanna C. S. Santos


+ [ Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go  Indifferent](https://arxiv.org//abs/2311.01205)

	Lorenz Kummer, Samir Moustafa, Nils N. Kriege, Wilfried N. Gansterer


+ [ Towards Evaluating Transfer-based Attacks Systematically, Practically,  and Fairly](https://arxiv.org//abs/2311.01323)

	Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen


+ [ Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game](https://arxiv.org//abs/2311.01011)

	Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, Stuart Russell


+ [ In Defense of Softmax Parametrization for Calibrated and Consistent  Learning to Defer](https://arxiv.org//abs/2311.01106)

	Yuzhou Cao, Hussein Mozannar, Lei Feng, Hongxin Wei, Bo An


+ [ MIST: Defending Against Membership Inference Attacks Through  Membership-Invariant Subspace Training](https://arxiv.org//abs/2311.00919)

	Jiacheng Li, Ninghui Li, Bruno Ribeiro


## 2023-11-1
+ [ FAIRLABEL: Correcting Bias in Labels](https://arxiv.org//abs/2311.00638)

	Srinivasan H Sengamedu, Hien Pham


+ [ Probing Explicit and Implicit Gender Bias through LLM Conditional Text  Generation](https://arxiv.org//abs/2311.00306)

	Xiangjue Dong, Yibo Wang, Philip S. Yu, James Caverlee


+ [ Robustness Tests for Automatic Machine Translation Metrics with  Adversarial Attacks](https://arxiv.org//abs/2311.00508)

	Yichen Huang, Timothy Baldwin


+ [ Medi-CAT: Contrastive Adversarial Training for Medical Image  Classification](https://arxiv.org//abs/2311.00154)

	Pervaiz Iqbal Khan, Andreas Dengel, Sheraz Ahmed


+ [ Uncertainty quantification and out-of-distribution detection using  surjective normalizing flows](https://arxiv.org//abs/2311.00377)

	Simon Dirmeier, Ye Hong, Yanan Xin, Fernando Perez-Cruz


+ [ NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust  Multi-Exit Neural Networks](https://arxiv.org//abs/2311.00428)

	Seokil Ham, Jungwuk Park, Dong-Jun Han, Jaekyun Moon


## 2023-10-31
+ [ Unmasking Bias and Inequities: A Systematic Review of Bias Detection and  Mitigation in Healthcare Artificial Intelligence Using Electronic Health  Records](https://arxiv.org//abs/2310.19917)

	Feng Chen, Liqin Wang, Julie Hong, Jiaqi Jiang, Li Zhou


+ [ Is Robustness Transferable across Languages in Multilingual Neural  Machine Translation?](https://arxiv.org//abs/2310.20162)

	Leiyu Pan, Supryadi, Deyi Xiong


+ [ LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B](https://arxiv.org//abs/2310.20624)

	Simon Lermen, Charlie Rogers-Smith, Jeffrey Ladish


+ [ DEPN: Detecting and Editing Privacy Neurons in Pretrained Language  Models](https://arxiv.org//abs/2310.20138)

	Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, Deyi Xiong


+ [ Verification of Neural Networks Local Differential Classification  Privacy](https://arxiv.org//abs/2310.20299)

	Roie Reshef, Anan Kabaha, Olga Seleznova, Dana Drachsler-Cohen


+ [ Initialization Matters: Privacy-Utility Analysis of Overparameterized  Neural Networks](https://arxiv.org//abs/2310.20579)

	Jiayuan Ye, Zhenyu Zhu, Fanghui Liu, Reza Shokri, Volkan Cevher


## 2023-10-30
+ [ PriPrune: Quantifying and Preserving Privacy in Pruned Federated  Learning](https://arxiv.org//abs/2310.19958)

	Tianyue Chu, Mengwei Yang, Nikolaos Laoutaris, Athina Markopoulou


+ [ LipSim: A Provably Robust Perceptual Similarity Metric](https://arxiv.org//abs/2310.18274)

	Sara Ghazanfari, Alexandre Araujo, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg


+ [ Counterfactual Fairness for Predictions using Generative Adversarial  Networks](https://arxiv.org//abs/2310.17687)

	Yuchen Ma, Dennis Frauen, Valentyn Melnychuk, Stefan Feuerriegel


## 2023-10-29
+ [ Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection  Method](https://arxiv.org//abs/2310.17918)

	Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Chong Meng, Shuaiqiang Wang, Zhicong Cheng, Zhaochun Ren, Dawei Yin


+ [ Fine tuning Pre trained Models for Robustness Under Noisy Labels](https://arxiv.org//abs/2310.17668)

	Sumyeong Ahn, Sihyeon Kim, Jongwoo Ko, Se-Young Yun


+ [ Adversarial Anomaly Detection using Gaussian Priors and Nonlinear  Anomaly Scores](https://arxiv.org//abs/2310.18091)

	Fiete Lüer, Tobias Weber, Maxim Dolgich, Christian Böhm


+ [ $α$-Mutual Information: A Tunable Privacy Measure for Privacy  Protection in Data Sharing](https://arxiv.org//abs/2310.18241)

	MirHamed Jafarzadeh Asl, Mohammadhadi Shateri, Fabrice Labeau


+ [ BlackJack: Secure machine learning on IoT devices through hardware-based  shuffling](https://arxiv.org//abs/2310.17804)

	Karthik Ganesan, Michal Fishkin, Ourong Lin, Natalie Enright Jerger



## 2023-10-27
+ [ ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in  Real-World User-AI Conversation](https://arxiv.org//abs/2310.17389)

	Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, Jingbo Shang


+ [ Where you go is who you are -- A study on machine learning based  semantic privacy attacks](https://arxiv.org//abs/2310.17643)

	Nina Wiedemann, Ourania Kounadi, Martin Raubal, Krzysztof Janowicz


+ [ A near-autonomous and incremental intrusion detection system through  active learning of known and unknown attacks](https://arxiv.org//abs/2310.17430)

	Lynda Boukela, Gongxuan Zhang, Meziane Yacoub, Samia Bouzefrane


## 2023-10-26
+ [ CBD: A Certified Backdoor Detector Based on Local Dominant Probability](https://arxiv.org//abs/2310.17498)

	Zhen Xiang, Zidi Xiong, Bo Li


+ [ A Survey on Transferability of Adversarial Examples across Deep Neural  Networks](https://arxiv.org//abs/2310.17626)

	Jindong Gu, Xiaojun Jia, Pau de Jorge, Wenqain Yu, Xinwei Liu, Avery Ma, Yuan Xun, Anjun Hu, Ashkan Khakzar, Zhijiang Li, Xiaochun Cao, Philip Torr


+ [ Uncertainty-weighted Loss Functions for Improved Adversarial Attacks on  Semantic Segmentation](https://arxiv.org//abs/2310.17436)

	Kira Maag, Asja Fischer


+ [ Detecting stealthy cyberattacks on adaptive cruise control vehicles: A  machine learning approach](https://arxiv.org//abs/2310.17091)

	Tianyi Li, Mingfeng Shang, Shian Wang, Raphael Stern


+ [ SoK: Pitfalls in Evaluating Black-Box Attacks](https://arxiv.org//abs/2310.17534)

	Fnu Suya, Anshuman Suri, Tingwei Zhang, Jingtao Hong, Yuan Tian, David Evans


+ [ Defending Against Transfer Attacks From Public Models](https://arxiv.org//abs/2310.17645)

	Chawin Sitawarin, Jaewon Chang, David Huang, Wesson Altoyan, David Wagner


+ [ Proving Test Set Contamination in Black Box Language Models](https://arxiv.org//abs/2310.17623)

	Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, Tatsunori B. Hashimoto


+ [ Detection Defenses: An Empty Promise against Adversarial Patch Attacks  on Optical Flow](https://arxiv.org//abs/2310.17403)

	Erik Scheurer, Jenny Schmalfuss, Alexander Lis, Andrés Bruhn


## 2023-10-25
+ [ Trust, but Verify: Robust Image Segmentation using Deep Learning](https://arxiv.org//abs/2310.16999)

	Fahim Ahmed Zaman, Xiaodong Wu, Weiyu Xu, Milan Sonka, Raghuraman Mudumbai


+ [ Break it, Imitate it, Fix it: Robustness by Generating Human-Like  Attacks](https://arxiv.org//abs/2310.16955)

	Aradhana Sinha, Ananth Balashankar, Ahmad Beirami, Thi Avrahami, Jilin Chen, Alex Beutel


+ [ Improving Few-shot Generalization of Safety Classifiers via Data  Augmented Parameter-Efficient Fine-Tuning](https://arxiv.org//abs/2310.16959)

	Ananth Balashankar, Xiao Ma, Aradhana Sinha, Ahmad Beirami, Yao Qin, Jilin Chen, Alex Beutel


+ [ Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained  Large Models Fine-Tuning](https://arxiv.org//abs/2310.16062)

	Shuoran Jiang, Qingcai Chen, Yang Xiang, Youcheng Pan, Xiangping Wu


## 2023-10-24
+ [ Enhancing Large Language Models for Secure Code Generation: A  Dataset-driven Study on Vulnerability Mitigation](https://arxiv.org//abs/2310.16263)

	Jiexin Wang, Liuwen Cao, Xitong Luo, Zhiping Zhou, Jiayuan Xie, Adam Jatowt, Yi Cai


+ [ Segue: Side-information Guided Generative Unlearnable Examples for  Facial Privacy Protection in Real World](https://arxiv.org//abs/2310.16061)

	Zhiling Zhang, Jie Zhang, Kui Zhang, Wenbo Zhou, Weiming Zhang, Nenghai Yu


+ [ Defense Against Model Extraction Attacks on Recommender Systems](https://arxiv.org//abs/2310.16335)

	Sixiao Zhang, Hongzhi Yin, Hongxu Chen, Cheng Long


+ [ Robust and Actively Secure Serverless Collaborative Learning](https://arxiv.org//abs/2310.16678)

	Olive Franzese, Adam Dziedzic, Christopher A. Choquette-Choo, Mark R. Thomas, Muhammad Ahmad Kaleem, Stephan Rabanser, Congyu Fang, Somesh Jha, Nicolas Papernot, Xiao Wang


+ [ AI Hazard Management: A framework for the systematic management of root  causes for AI risks](https://arxiv.org//abs/2310.16727)

	Ronald Schnitzer, Andreas Hapfelmeier, Sven Gaube, Sonja Zillner


+ [ FLTrojan: Privacy Leakage Attacks against Federated Language Models  Through Selective Weight Tampering](https://arxiv.org//abs/2310.16152)

	Md Rafi Ur Rashid, Vishnu Asutosh Dasu, Kang Gu, Najrin Sultana, Shagufta Mehnaz


+ [ Poison is Not Traceless: Fully-Agnostic Detection of Poisoning Attacks](https://arxiv.org//abs/2310.16224)

	Xinglong Chang, Katharina Dost, Gillian Dobbie, Jörg Wicker


+ [ A model for multi-attack classification to improve intrusion detection  performance using deep learning approaches](https://arxiv.org//abs/2310.16380)

	Arun Kumar Silivery, Ram Mohan Rao Kovvur


## 2023-10-23
+ [ 3D Masked Autoencoders for Enhanced Privacy in MRI Scans](https://arxiv.org//abs/2310.15778)

	Lennart Alexander Van der Goten, Kevin Smith


+ [ Self-Guard: Empower the LLM to Safeguard Itself](https://arxiv.org//abs/2310.15851)

	Zezhong Wang, Fangkai Yang, Lu Wang, Pu Zhao, Hongru Wang, Liang Chen, Qingwei Lin, Kam-Fai Wong


+ [ The Janus Interface: How Fine-Tuning in Large Language Models Amplifies  the Privacy Risks](https://arxiv.org//abs/2310.15469)

	Xiaoyi Chen, Siyuan Tang, Rui Zhu, Shijun Yan, Lei Jin, Zihao Wang, Liya Su, XiaoFeng Wang, Haixu Tang


+ [ Deceptive Fairness Attacks on Graphs via Meta Learning](https://arxiv.org//abs/2310.15653)

	Jian Kang, Yinglong Xia, Ross Maciejewski, Jiebo Luo, Hanghang Tong


+ [ Momentum Gradient-based Untargeted Attack on Hypergraph Neural Networks](https://arxiv.org//abs/2310.15656)

	Yang Chen, Stjepan Picek, Zhonglin Ye, Zhaoyang Wang, Haixing Zhao


## 2023-10-22
+ [ Fundamental Limits of Membership Inference Attacks on Machine Learning  Models](https://arxiv.org//abs/2310.13786)

	Eric Aubinais, Elisabeth Gassiat, Pablo Piantanida


+ [ Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models](https://arxiv.org//abs/2310.13828)

	Shawn Shan, Wenxin Ding, Josephine Passananti, Haitao Zheng, Ben Y. Zhao


+ [ MoPe: Model Perturbation-based Privacy Attacks on Language Models](https://arxiv.org//abs/2310.14369)

	Marvin Li, Jason Wang, Jeffrey Wang, Seth Neel


+ [ On existence, uniqueness and scalability of adversarial robustness  measures for AI classifiers](https://arxiv.org//abs/2310.14421)

	Illia Horenko


+ [ AutoDAN: Automatic and Interpretable Adversarial Attacks on Large  Language Models](https://arxiv.org//abs/2310.15140)

	Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, Tong Sun


+ [ Toward Stronger Textual Attack Detectors](https://arxiv.org//abs/2310.14001)

	Pierre Colombo, Marine Picot, Nathan Noiry, Guillaume Staerman, Pablo Piantanida


+ [ CT-GAT: Cross-Task Generative Adversarial Attack based on  Transferability](https://arxiv.org//abs/2310.14265)

	Minxuan Lv, Chengwei Dai, Kun Li, Wei Zhou, Songlin Hu


+ [ Data-Free Knowledge Distillation Using Adversarially Perturbed OpenGL  Shader Images](https://arxiv.org//abs/2310.13782)

	Logan Frank, Jim Davis


+ [ Bi-discriminator Domain Adversarial Neural Networks with Class-Level  Gradient Alignment](https://arxiv.org//abs/2310.13959)

	Chuang Zhao, Hongke Zhao, Hengshu Zhu, Zhenya Huang, Nan Feng, Enhong Chen, Hui Xiong


+ [ ADoPT: LiDAR Spoofing Attack Detection Based on Point-Level Temporal  Consistency](https://arxiv.org//abs/2310.14504)

	Minkyoung Cho, Yulong Cao, Zixiang Zhou, Z. Morley Mao


+ [ F$^2$AT: Feature-Focusing Adversarial Training via Disentanglement of  Natural and Perturbed Patterns](https://arxiv.org//abs/2310.14561)

	Yaguan Qian, Chenyu Zhao, Zhaoquan Gu, Bin Wang, Shouling Ji, Wei Wang, Boyang Zhou, Pan Zhou


+ [ Semantic-Aware Adversarial Training for Reliable Deep Hashing Retrieval](https://arxiv.org//abs/2310.14637)

	Xu Yuan, Zheng Zhang, Xunguang Wang, Lin Wu


+ [ On the Detection of Image-Scaling Attacks in Machine Learning](https://arxiv.org//abs/2310.15085)

	Erwin Quiring, Andreas Müller, Konrad Rieck


+ [ Adversarial Attacks on Fairness of Graph Neural Networks](https://arxiv.org//abs/2310.13822)

	Binchi Zhang, Yushun Dong, Chen Chen, Yada Zhu, Minnan Luo, Jundong Li


+ [ Competitive Advantage Attacks to Decentralized Federated Learning](https://arxiv.org//abs/2310.13862)

	Yuqi Jia, Minghong Fang, Neil Zhenqiang Gong


+ [ Enhancing Accuracy-Privacy Trade-off in Differentially Private Split  Learning](https://arxiv.org//abs/2310.14434)

	Ngoc Duy Pham, Khoa Tran Phan, Naveen Chilamkurti


## 2023-10-21
+ [ GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for  Reasoning Problems](https://arxiv.org//abs/2310.12397)

	Kaya Stechly, Matthew Marquez, Subbarao Kambhampati


+ [ Prompt Injection Attacks and Defenses in LLM-Integrated Applications](https://arxiv.org//abs/2310.12815)

	Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang Gong


+ [ Probing LLMs for hate speech detection: strengths and vulnerabilities](https://arxiv.org//abs/2310.12860)

	Sarthak Roy, Ashish Harshavardhan, Animesh Mukherjee, Punyajoy Saha


## 2023-10-20
+ [ PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models](https://arxiv.org//abs/2310.12439)

	Hongwei Yao, Jian Lou, Zhan Qin


+ [ Segment Anything Meets Universal Adversarial Perturbation](https://arxiv.org//abs/2310.12431)

	Dongshen Han, Sheng Zheng, Chaoning Zhang


+ [ Fast Model Debias with Machine Unlearning](https://arxiv.org//abs/2310.12560)

	Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, Yang Feng, Joey Tianyi Zhou, Jian Wu, Zuozhu Liu


## 2023-10-19
+ [ Automatic Hallucination Assessment for Aligned Large Language Models via  Transferable Adversarial Attacks](https://arxiv.org//abs/2310.12516)

	Xiaodong Yu, Hao Cheng, Xiaodong Liu, Dan Roth, Jianfeng Gao


+ [ Attack Prompt Generation for Red Teaming and Defending Large Language  Models](https://arxiv.org//abs/2310.12505)

	Boyi Deng, Wenjie Wang, Fuli Feng, Yang Deng, Qifan Wang, Xiangnan He


+ [ Recoverable Privacy-Preserving Image Classification through Noise-like  Adversarial Examples](https://arxiv.org//abs/2310.12707)

	Jun Liu, Jiantao Zhou, Jinyu Tian, Weiwei Sun


+ [ REVAMP: Automated Simulations of Adversarial Attacks on Arbitrary  Objects in Realistic Scenes](https://arxiv.org//abs/2310.12243)

	Matthew Hull, Zijie J. Wang, Duen Horng Chau


+ [ Generating Robust Adversarial Examples against Online Social Networks  (OSNs)](https://arxiv.org//abs/2310.12708)

	Jun Liu, Jiantao Zhou, Haiwei Wu, Weiwei Sun, Jinyu Tian


+ [ OODRobustBench: benchmarking and analyzing adversarial robustness under  distribution shift](https://arxiv.org//abs/2310.12793)

	Lin Li, Yifei Wang, Chawin Sitawarin, Michael Spratling


+ [ CAT: Closed-loop Adversarial Training for Safe End-to-End Driving](https://arxiv.org//abs/2310.12432)

	Linrui Zhang, Zhenghao Peng, Quanyi Li, Bolei Zhou


+ [ Knowledge from Uncertainty in Evidential Deep Learning](https://arxiv.org//abs/2310.12663)

	Cai Davies, Marc Roig Vilamala, Alun D. Preece, Federico Cerutti, Lance M. Kaplan, Supriyo Chakraborty


+ [ Learn from the Past: A Proxy based Adversarial Defense Framework to  Boost Robustness](https://arxiv.org//abs/2310.12713)

	Yaohua Liu, Jiaxin Gao, Zhu Liu, Xianghao Jiao, Xin Fan, Risheng Liu


+ [ PrivInfer: Privacy-Preserving Inference for Black-box Large Language  Model](https://arxiv.org//abs/2310.12214)

	Meng Tong, Kejiang Chen, Yuang Qi, Jie Zhang, Weiming Zhang, Nenghai Yu


+ [ Privacy Preserving Large Language Models: ChatGPT Case Study Based  Vision and Framework](https://arxiv.org//abs/2310.12523)

	Imdad Ullah, Najm Hassan, Sukhpal Singh Gill, Basem Suleiman, Tariq Ahamed Ahanger, Zawar Shah, Junaid Qadir, Salil S. Kanhere


+ [ SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models](https://arxiv.org//abs/2310.12665)

	Boyang Zhang, Zheng Li, Ziqing Yang, Xinlei He, Michael Backes, Mario Fritz, Yang Zhang


## 2023-10-18
+ [ Adversarial Robustness Unhardening via Backdoor Attacks in Federated  Learning](https://arxiv.org//abs/2310.11594)

	Taejin Kim, Jiarui Li, Shubhranshu Singh, Nikhil Madaan, Carlee Joe-Wong


+ [ WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks  Against Deep Neural Networks](https://arxiv.org//abs/2310.11595)

	Jun Xia, Zhihao Yue, Yingbo Zhou, Zhiwei Ling, Xian Wei, Mingsong Chen


+ [ The Efficacy of Transformer-based Adversarial Attacks in Security  Domains](https://arxiv.org//abs/2310.11597)

	Kunyang Li, Kyle Domico, Jean-Charles Noirot Ferrand, Patrick McDaniel


+ [ Black-Box Training Data Identification in GANs via Detector Networks](https://arxiv.org//abs/2310.12063)

	Lukman Olagoke, Salil Vadhan, Seth Neel


+ [ A Cautionary Tale: On the Role of Reference Data in Empirical Privacy  Defenses](https://arxiv.org//abs/2310.12112)

	Caelin G. Kaplan, Chuan Xu, Othmane Marfoq, Giovanni Neglia, Anderson Santana de Oliveira


+ [ Domain-Generalized Face Anti-Spoofing with Unknown Attacks](https://arxiv.org//abs/2310.11758)

	Zong-Wei Hong, Yu-Chen Lin, Hsuan-Tung Liu, Yi-Ren Yeh, Chu-Song Chen


+ [ To Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still  Easy To Generate Unsafe Images ... For Now](https://arxiv.org//abs/2310.11868)

	Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, Sijia Liu


+ [ Exploring Decision-based Black-box Attacks on Face Forgery Detection](https://arxiv.org//abs/2310.12017)

	Zhaoyu Chen, Bo Li, Kaixun Jiang, Shuang Wu, Shouhong Ding, Wenqiang Zhang


+ [ Revisiting Transferable Adversarial Image Examples: Attack  Categorization, Evaluation Guidelines, and New Insights](https://arxiv.org//abs/2310.11850)

	Zhengyu Zhao, Hanwei Zhang, Renjue Li, Ronan Sicre, Laurent Amsaleg, Michael Backes, Qi Li, Chao Shen


+ [ In defense of parameter sharing for model-compression](https://arxiv.org//abs/2310.11611)

	Aditya Desai, Anshumali Shrivastava


+ [ Adversarial Training for Physics-Informed Neural Networks](https://arxiv.org//abs/2310.11789)

	Yao Li, Shengzhu Shi, Zhichang Guo, Boying Wu


+ [ Quantifying Privacy Risks of Prompts in Visual Prompt Learning](https://arxiv.org//abs/2310.11970)

	Yixin Wu, Rui Wen, Michael Backes, Pascal Berrang, Mathias Humbert, Yun Shen, Yang Zhang



## 2023-10-17
+ [ Demystifying Poisoning Backdoor Attacks from a Statistical Perspective](https://arxiv.org//abs/2310.10780)

	Xun Xian, Ganghua Wang, Jayanth Srinivasa, Ashish Kundu, Xuan Bi, Mingyi Hong, Jie Ding


+ [ Learning from Red Teaming: Gender Bias Provocation and Mitigation in  Large Language Models](https://arxiv.org//abs/2310.11079)

	Hsuan Su, Cheng-Chu Cheng, Hua Farn, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee


+ [ Quantifying Language Models' Sensitivity to Spurious Features in Prompt  Design or: How I learned to start worrying about prompt formatting](https://arxiv.org//abs/2310.11324)

	Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr


+ [ Functional Invariants to Watermark Large Transformers](https://arxiv.org//abs/2310.11446)

	Fernandez Pierre, Couairon Guillaume, Furon Teddy, Douze Matthijs


+ [ Fake News in Sheep's Clothing: Robust Fake News Detection Against  LLM-Empowered Style Attacks](https://arxiv.org//abs/2310.10830)

	Jiaying Wu, Bryan Hooi


+ [ Survey of Vulnerabilities in Large Language Models Revealed by  Adversarial Attacks](https://arxiv.org//abs/2310.10844)

	Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, Nael Abu-Ghazaleh


+ [ Will the Prince Get True Love's Kiss? On the Model Sensitivity to Gender  Perturbation over Fairytale Texts](https://arxiv.org//abs/2310.10865)

	Christina Chance, Da Yin, Dakuo Wang, Kai-Wei Chang


+ [ Backdoor Attack through Machine Unlearning](https://arxiv.org//abs/2310.10659)

	Peixin Zhang, Jun Sun, Mingtian Tan, Xinyu Wang


+ [ Regularization properties of adversarially-trained linear regression](https://arxiv.org//abs/2310.10807)

	Antônio H. Ribeiro, Dave Zachariah, Francis Bach, Thomas B. Schön


+ [ Locally Differentially Private Graph Embedding](https://arxiv.org//abs/2310.11060)

	Zening Li, Rong-Hua Li, Meihao Liao, Fusheng Jin, Guoren Wang


+ [ Last One Standing: A Comparative Analysis of Security and Privacy of  Soft Prompt Tuning, LoRA, and In-Context Learning](https://arxiv.org//abs/2310.11397)

	Rui Wen, Tianhao Wang, Michael Backes, Yang Zhang, Ahmed Salem


+ [ Unbiased Watermark for Large Language Models](https://arxiv.org//abs/2310.10669)

	Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, Heng Huang


## 2023-10-16
+ [ DANAA: Towards transferable attacks with double adversarial neuron  attribution](https://arxiv.org//abs/2310.10427)

	Zhibo Jin, Zhiyu Zhu, Xinyi Wang, Jiayu Zhang, Jun Shen, Huaming Chen


+ [ Privacy in Large Language Models: Attacks, Defenses and Future  Directions](https://arxiv.org//abs/2310.10383)

	Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, Yangqiu Song


+ [ Prompt Packer: Deceiving LLMs through Compositional Instruction with  Hidden Attacks](https://arxiv.org//abs/2310.10077)

	Shuyu Jiang, Xingshu Chen, Rui Tang


+ [ ASSERT: Automated Safety Scenario Red Teaming for Evaluating the  Robustness of Large Language Models](https://arxiv.org//abs/2310.09624)

	Alex Mei, Sharon Levy, William Yang Wang


+ [ Orthogonal Uncertainty Representation of Data Manifold for Robust  Long-Tailed Learning](https://arxiv.org//abs/2310.10090)

	Yanbiao Ma, Licheng Jiao, Fang Liu, Shuyuan Yang, Xu Liu, Lingling Li


+ [ Quantifying Assistive Robustness Via the Natural-Adversarial Frontier](https://arxiv.org//abs/2310.10610)

	Jerry Zhi-Yang He, Zackory Erickson, Daniel S. Brown, Anca D. Dragan


## 2023-10-15
+ [ SCME: A Self-Contrastive Method for Data-free and Query-Limited Model  Extraction Attack](https://arxiv.org//abs/2310.09792)

	Renyang Liu, Jinhong Zhang, Kwok-Yan Lam, Jun Zhao, Wei Zhou


+ [ AFLOW: Developing Adversarial Examples under Extremely Noise-limited  Settings](https://arxiv.org//abs/2310.09795)

	Renyang Liu, Jinhong Zhang, Haoran Li, Jin Zhang, Yuanyu Wang, Wei Zhou


+ [ Black-box Targeted Adversarial Attack on Segment Anything (SAM)](https://arxiv.org//abs/2310.10010)

	Sheng Zheng, Chaoning Zhang


+ [ Explore the Effect of Data Selection on Poison Efficiency in Backdoor  Attacks](https://arxiv.org//abs/2310.09744)

	Ziqiang Li, Pengfei Xia, Hong Sun, Yueqi Zeng, Wei Zhang, Bin Li


+ [ Model Inversion Attacks on Homogeneous and Heterogeneous Graph Neural  Networks](https://arxiv.org//abs/2310.09800)

	Renyang Liu, Wei Zhou, Jinhong Zhang, Xiaoyuan Liu, Peiyuan Si, Haoran Li


+ [ Evaluating Robustness of Visual Representations for Object Assembly Task  Requiring Spatio-Geometrical Reasoning](https://arxiv.org//abs/2310.09943)

	Chahyon Ku, Carl Winge, Ryan Diaz, Wentao Yuan, Karthik Desingh


+ [ Is Certifying $\ell_p$ Robustness Still Worthwhile?](https://arxiv.org//abs/2310.09361)

	Ravi Mangal, Klas Leino, Zifan Wang, Kai Hu, Weicheng Yu, Corina Pasareanu, Anupam Datta, Matt Fredrikson


+ [ MAGIC: Detecting Advanced Persistent Threats via Masked Graph  Representation Learning](https://arxiv.org//abs/2310.09831)

	Zian Jia, Yun Xiong, Yuhong Nan, Yao Zhang, Jinjing Zhao, Mi Wen


+ [ A Comprehensive Study of Privacy Risks in Curriculum Learning](https://arxiv.org//abs/2310.10124)

	Joann Qiongna Chen, Xinlei He, Zheng Li, Yang Zhang, Zhou Li


+ [ Prime Match: A Privacy-Preserving Inventory Matching System](https://arxiv.org//abs/2310.09621)

	Antigoni Polychroniadou, Gilad Asharov, Benjamin Diamond, Tucker Balch, Hans Buehler, Richard Hua, Suwen Gu, Greg Gimler, Manuela Veloso


+ [ BufferSearch: Generating Black-Box Adversarial Texts With Lower Queries](https://arxiv.org//abs/2310.09652)

	Wenjie Lv, Zhen Wang, Yitao Zheng, Zhehua Zhong, Qi Xuan, Tianyi Chen


## 2023-10-14
+ [ Defending Our Privacy With Backdoors](https://arxiv.org//abs/2310.08320)

	Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting


+ [ Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural  Networks](https://arxiv.org//abs/2310.08073)

	Giorgio Piras, Maura Pintor, Ambra Demontis, Battista Biggio


## 2023-10-13
+ [ Towards the Vulnerability of Watermarking Artificial Intelligence  Generated Content](https://arxiv.org//abs/2310.07726)

	Guanlin Li, Yifei Chen, Jie Zhang, Jiwei Li, Shangwei Guo, Tianwei Zhang


+ [ Sentinel: An Aggregation Function to Secure Decentralized Federated  Learning](https://arxiv.org//abs/2310.08097)

	Chao Feng, Alberto Huertas Celdran, Janosch Baltensperger, Enrique Tomas Matınez Bertran, Gerome Bovet, Burkhard Stiller


+ [ Trustworthy Machine Learning](https://arxiv.org//abs/2310.08215)

	Bálint Mucsányi, Michael Kirchhof, Elisa Nguyen, Alexander Rubinstein, Seong Joon Oh


+ [ Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders](https://arxiv.org//abs/2310.08571)

	Jan Dubiński, Stanisław Pawlak, Franziska Boenisch, Tomasz Trzciński, Adam Dziedzic

## 2023-10-12
+ [ Effects of Human Adversarial and Affable Samples on BERT  Generalizability](https://arxiv.org//abs/2310.08008)

	Aparna Elangovan, Jiayuan He, Yuan Li, Karin Verspoor


+ [ Concealed Electronic Countermeasures of Radar Signal with Adversarial  Examples](https://arxiv.org//abs/2310.08292)

	Ruinan Ma, Canjie Zhu, Mingfeng Lu, Yunjie Li, Yu-an Tan, Ruibin Zhang, Ran Tao


+ [ Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org//abs/2310.08419)

	Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong


+ [ Towards Robust Multi-Modal Reasoning via Model Selection](https://arxiv.org//abs/2310.08446)

	Xiangyan Liu, Rongxue Li, Wei Ji, Tao Lin


+ [ Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization](https://arxiv.org//abs/2310.08177)

	Giuseppe Floris, Raffaele Mura, Luca Scionis, Giorgio Piras, Maura Pintor, Ambra Demontis, Battista Biggio


+ [ Invisible Threats: Backdoor Attack in OCR Systems](https://arxiv.org//abs/2310.08259)

	Mauro Conti, Nicola Farronato, Stefanos Koffas, Luca Pajola, Stjepan Picek


+ [ Promoting Robustness of Randomized Smoothing: Two Cost-Effective  Approaches](https://arxiv.org//abs/2310.07780)

	Linbo Liu, Trong Nghia Hoang, Lam M. Nguyen, Tsui-Wei Weng


+ [ Towards Causal Deep Learning for Vulnerability Detection](https://arxiv.org//abs/2310.07958)

	Md Mahbubur Rahman, Ira Ceka, Chengzhi Mao, Saikat Chakraborty, Baishakhi Ray, Wei Le


## 2023-10-11
+ [ RobustEdge: Low Power Adversarial Detection for Cloud-Edge Systems](https://arxiv.org//abs/2310.06845)

	Abhishek Moitra, Abhiroop Bhattacharjee, Youngeun Kim, Priyadarshini Panda


+ [ Genetic Algorithm-Based Dynamic Backdoor Attack on Federated  Learning-Based Network Traffic Classification](https://arxiv.org//abs/2310.06855)

	Mahmoud Nazzal, Nura Aljaafari, Ahmed Sawalmeh, Abdallah Khreishah, Muhammad Anan, Abdulelah Algosaibi, Mohammed Alnaeem, Adel Aldalbahi, Abdulaziz Alhumam, Conrado P. Vizcarra, Shadan Alhamed


+ [ Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation](https://arxiv.org//abs/2310.06987)

	Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, Danqi Chen


+ [ No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN  Partition for On-Device ML](https://arxiv.org//abs/2310.07152)

	Ziqi Zhang, Chen Gong, Yifeng Cai, Yuanyuan Yuan, Bingyan Liu, Ding Li, Yao Guo, Xiangqun Chen


+ [ Boosting Black-box Attack to Deep Neural Networks with Conditional  Diffusion Models](https://arxiv.org//abs/2310.07492)

	Renyang Liu, Wei Zhou, Tianwei Zhang, Kangjie Chen, Jun Zhao, Kwok-Yan Lam


+ [ Composite Backdoor Attacks Against Large Language Models](https://arxiv.org//abs/2310.07676)

	Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang


+ [ Comparing the robustness of modern no-reference image- and video-quality  metrics to adversarial attacks](https://arxiv.org//abs/2310.06958)

	Anastasia Antsiferova, Khaled Abud, Aleksandr Gushchin, Sergey Lavrushkin, Ekaterina Shumitskaya, Maksim Velikanov, Dmitriy Vatolin


+ [ Robust Safe Reinforcement Learning under Adversarial Disturbances](https://arxiv.org//abs/2310.07207)

	Zeyang Li, Chuxiong Hu, Shengbo Eben Li, Jia Cheng, Yunan Wang


## 2023-10-10
+ [ Fingerprint Attack: Client De-Anonymization in Federated Learning](https://arxiv.org//abs/2310.05960)

	Qiongkai Xu, Trevor Cohn, Olga Ohrimenko


+ [ Suppressing Overestimation in Q-Learning through Adversarial Behaviors](https://arxiv.org//abs/2310.06286)

	HyeAnn Lee, Donghwan Lee


+ [ Jailbreak and Guard Aligned Language Models with Only Few In-Context  Demonstrations](https://arxiv.org//abs/2310.06387)

	Zeming Wei, Yifei Wang, Yisen Wang


+ [ Multilingual Jailbreak Challenges in Large Language Models](https://arxiv.org//abs/2310.06474)

	Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing


+ [ A Semantic Invariant Robust Watermark for Large Language Models](https://arxiv.org//abs/2310.06356)

	Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, Lijie Wen


+ [ Adversarial Masked Image Inpainting for Robust Detection of Mpox and  Non-Mpox](https://arxiv.org//abs/2310.06318)

	Yubiao Yue, Zhenzhang Li


+ [ Leveraging Diffusion-Based Image Variations for Robust Training on  Poisoned Data](https://arxiv.org//abs/2310.06372)

	Lukas Struppek, Martin B. Hentschel, Clifton Poth, Dominik Hintersdorf, Kristian Kersting


+ [ Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield  but Also a Catalyst for Model Inversion Attacks](https://arxiv.org//abs/2310.06549)

	Lukas Struppek, Dominik Hintersdorf, Kristian Kersting


+ [ Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK  Approach](https://arxiv.org//abs/2310.06112)

	Shaopeng Fu, Di Wang


+ [ PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust  Generalization](https://arxiv.org//abs/2310.06182)

	Jiancong Xiao, Ruoyu Sun, Zhi-quan Luo


+ [ Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach](https://arxiv.org//abs/2310.06396)

	Kai Zhao, Qiyu Kang, Yang Song, Rui She, Sijie Wang, Wee Peng Tay


+ [ Exploring adversarial attacks in federated learning for medical imaging](https://arxiv.org//abs/2310.06227)

	Erfan Darzi, Florian Dubost, N.M. Sijtsema, P.M.A van Ooijen


## 2023-10-09

+ [ The Troubling Emergence of Hallucination in Large Language Models -- An  Extensive Definition, Quantification, and Prescriptive Remediations](https://arxiv.org//abs/2310.04988)

	Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das


+ [ Large Language Models Can Be Good Privacy Protection Learners](https://arxiv.org//abs/2310.02469)

	Yijia Xiao, Yiqiao Jin, Yushi Bai, Yue Wu, Xianjun Yang, Xiao Luo, Wenchao Yu, Xujiang Zhao, Yanchi Liu, Haifeng Chen, Wei Wang, Wei Cheng


+ [ LoFT: Local Proxy Fine-tuning For Improving Transferability Of  Adversarial Attacks Against Large Language Model](https://arxiv.org//abs/2310.04445)

	Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, Raphael Olivier, Ankit Shah, Dareen Alharthi, Hazim T Bukhari, Massa Baali, Soham Deshmukh, Michael Kuhlmann, Bhiksha Raj, Rita Singh


+ [ AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language  Models](https://arxiv.org//abs/2310.04451)

	Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao



+ [ Understanding and Improving Adversarial Attacks on Latent Diffusion  Model](https://arxiv.org//abs/2310.04687)

	Boyang Zheng, Chumeng Liang, Xiaoyu Wu, Yan Liu


+ [ Robustness-enhanced Uplift Modeling with Adversarial Feature  Desensitization](https://arxiv.org//abs/2310.04693)

	Zexu Sun, Bowei He, Ming Ma, Jiakai Tang, Yuchen Wang, Chen Ma, Dugang Liu



+ [ Better Safe than Sorry: Pre-training CLIP against Targeted Data  Poisoning and Backdoor Attacks](https://arxiv.org//abs/2310.05862)

	Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman


+ [ BRAINTEASER: Lateral Thinking Puzzles for Large Language Model](https://arxiv.org//abs/2310.05057)

	Yifan Jiang, Filip Ilievski, Kaixin Ma


+ [ Do Large Language Models Know about Facts?](https://arxiv.org//abs/2310.05177)

	Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S. Yu, Zhijiang Guo


+ [ SC-Safety: A Multi-round Open-ended Question Adversarial Safety  Benchmark for Large Language Models in Chinese](https://arxiv.org//abs/2310.05818)

	Liang Xu, Kangkang Zhao, Lei Zhu, Hang Xue


+ [ IPMix: Label-Preserving Data Augmentation Method for Training Robust  Classifiers](https://arxiv.org//abs/2310.04780)

	Zhenglin Huang, Xianan Bao, Na Zhang, Qingqi Zhang, Xiaomei Tu, Biao Wu, Xi Yang


+ [ VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via  Pre-trained Models](https://arxiv.org//abs/2310.04655)

	Ziyi Yin, Muchao Ye, Tianrong Zhang, Tianyu Du, Jinguo Zhu, Han Liu, Jinghui Chen, Ting Wang, Fenglong Ma


+ [ GReAT: A Graph Regularized Adversarial Training Method](https://arxiv.org//abs/2310.05336)

	Samet Bayram, Kenneth Barner


+ [ Generating Less Certain Adversarial Examples Improves Robust  Generalization](https://arxiv.org//abs/2310.04539)

	Minxing Zhang, Michael Backes, Xiao Zhang


+ [ Protecting Sensitive Data through Federated Co-Training](https://arxiv.org//abs/2310.05696)

	Amr Abourayya, Jens Kleesiek, Kanishka Rao, Erman Ayday, Bharat Rao, Geoff Webb, Michael Kamp


+ [ Tight Certified Robustness via Min-Max Representations of ReLU Neural  Networks](https://arxiv.org//abs/2310.04916)

	Brendon G. Anderson, Samuel Pfrommer, Somayeh Sojoudi


## 2023-10-08
+ [ Lightweight Boosting Models for User Response Prediction Using  Adversarial Validation](https://arxiv.org//abs/2310.03778)

	Hyeonwoo Kim, Wonsung Lee


+ [ Assessing Robustness via Score-Based Adversarial Image Generation](https://arxiv.org//abs/2310.04285)

	Marcel Kollovieh, Lukas Gosch, Yan Scholten, Marten Lienen, Stephan Günnemann


+ [ Indirect Meltdown: Building Novel Side-Channel Attacks from  Transient-Execution Attacks](https://arxiv.org//abs/2310.04183)

	Daniel Weber, Fabian Thomas, Lukas Gerlach, Ruiyi Zhang, Michael Schwarz


## 2023-10-07
+ [ Benchmarking Local Robustness of High-Accuracy Binary Neural Networks  for Enhanced Traffic Sign Recognition](https://arxiv.org//abs/2310.03033)

	Andreea Postovan, Mădălina Eraşcu
	

+ [ Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models](https://arxiv.org//abs/2310.03123)

	Zihao Lin, Yan Sun, Yifan Shi, Xueqian Wang, Lifu Huang, Li Shen, Dacheng Tao


+ [ Fine-tuning Aligned Language Models Compromises Safety, Even When Users  Do Not Intend To!](https://arxiv.org//abs/2310.03693)

	Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson


+ [ Ask for Alice: Online Covert Distress Signal in the Presence of a Strong  Adversary](https://arxiv.org//abs/2310.03237)

	Hayyu Imanda, Kasper Rasmussen


## 2023-10-06
+ [ Misusing Tools in Large Language Models With Visual Adversarial Examples](https://arxiv.org//abs/2310.03185)

	Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, Earlence Fernandes


+ [ Robust Representation Learning via Asymmetric Negative Contrast and  Reverse Attention](https://arxiv.org//abs/2310.03358)

	Nuoyan Zhou, Decheng Liu, Dawei Zhou, Xinbo Gao, Nannan Wang


+ [ SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org//abs/2310.03684)

	Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas


+ [ A Formalism and Approach for Improving Robustness of Large Language  Models Using Risk-Adjusted Confidence Scores](https://arxiv.org//abs/2310.03283)

	Ke Shen, Mayank Kejriwal


+ [ Shielding the Unseen: Privacy Protection through Poisoning NeRF with  Spatial Deformation](https://arxiv.org//abs/2310.03125)

	Yihan Wu, Brandon Y. Feng, Heng Huang


+ [ CSI: Enhancing the Robustness of 3D Point Cloud Recognition against  Corruption](https://arxiv.org//abs/2310.03360)

	Zhuoyuan Wu, Jiachen Sun, Chaowei Xiao


+ [ OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable  Evasion Attacks](https://arxiv.org//abs/2310.03707)

	Ofir Bar Tal, Adi Haviv, Amit H. Bermano


+ [ Untargeted White-box Adversarial Attack with Heuristic Defence Methods  in Real-time Deep Learning based Network Intrusion Detection System](https://arxiv.org//abs/2310.03334)

	Khushnaseeb Roshan, Aasim Zafar, Sheikh Burhan Ul Haque


+ [ Targeted Adversarial Attacks on Generalizable Neural Radiance Fields](https://arxiv.org//abs/2310.03578)

	Andras Horvath, Csaba M. Jozsa


+ [ Adversarial Machine Learning for Social Good: Reframing the Adversary as  an Ally](https://arxiv.org//abs/2310.03614)

	Shawqi Al-Maliki, Adnan Qayyum, Hassan Ali, Mohamed Abdallah, Junaid Qadir, Dinh Thai Hoang, Dusit Niyato, Ala Al-Fuqaha


+ [ Raze to the Ground: Query-Efficient Adversarial HTML Attacks on  Machine-Learning Phishing Webpage Detectors](https://arxiv.org//abs/2310.03166)

	Biagio Montaruli, Luca Demetrio, Maura Pintor, Luca Compagna, Davide Balzarotti, Battista Biggio


+ [ Regret Analysis of Distributed Online Control for LTI Systems with  Adversarial Disturbances](https://arxiv.org//abs/2310.03206)

	Ting-Jui Chang, Shahin Shahrampour


+ [ Certifiably Robust Graph Contrastive Learning](https://arxiv.org//abs/2310.03312)

	Minhua Lin, Teng Xiao, Enyan Dai, Xiang Zhang, Suhang Wang


+ [ An Integrated Algorithm for Robust and Imperceptible Audio Adversarial  Examples](https://arxiv.org//abs/2310.03349)

	Armin Ettenhofer, Jan-Philipp Schulze, Karla Pizzi


## 2023-10-05
+ [ Low-Resource Languages Jailbreak GPT-4](https://arxiv.org//abs/2310.02446)

	Zheng-Xin Yong, Cristina Menghini, Stephen H. Bach


+ [ Discovering General Reinforcement Learning Algorithms with Adversarial  Environment Design](https://arxiv.org//abs/2310.02782)

	Matthew Thomas Jackson, Minqi Jiang, Jack Parker-Holder, Risto Vuorio, Chris Lu, Gregory Farquhar, Shimon Whiteson, Jakob Nicolaus Foerster


+ [ Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models](https://arxiv.org//abs/2310.02949)

	Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, Dahua Lin


+ [ Can Large Language Models Provide Security & Privacy Advice? Measuring  the Ability of LLMs to Refute Misconceptions](https://arxiv.org//abs/2310.02431)

	Yufan Chen, Arjun Arunasalam, Z. Berkay Celik


+ [ SlowFormer: Universal Adversarial Patch for Attack on Compute and Energy  Efficiency of Inference Efficient Vision Transformers](https://arxiv.org//abs/2310.02544)

	KL Navaneet, Soroush Abbasi Koohpayegani, Essam Sleiman, Hamed Pirsiavash


+ [ Splitting the Difference on Adversarial Training](https://arxiv.org//abs/2310.02480)

	Matan Levi, Aryeh Kontorovich


+ [ A Recipe for Improved Certifiable Robustness: Capacity and Data](https://arxiv.org//abs/2310.02513)

	Kai Hu, Klas Leino, Zifan Wang, Matt Fredrikson


+ [ Jailbreaker in Jail: Moving Target Defense for Large Language Models](https://arxiv.org//abs/2310.02417)

	Bocheng Chen, Advait Paliwal, Qiben Yan


## 2023-10-04
+ [ Identifying and Mitigating Privacy Risks Stemming from Language Models:  A Survey](https://arxiv.org//abs/2310.01424)

	Victoria Smith, Ali Shahin Shamsabadi, Carolyn Ashurst, Adrian Weller


+ [ Fooling the Textual Fooler via Randomizing Latent Representations](https://arxiv.org//abs/2310.01452)

	Duy C. Hoang, Quang H. Nguyen, Saurav Manchanda, MinLong Peng, Kok-Seng Wong, Khoa D. Doan


+ [ On the Safety of Open-Sourced Large Language Models: Does Alignment  Really Prevent Them From Being Misused?](https://arxiv.org//abs/2310.01581)

	Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu Lin, Jinyuan Jia, Jinghui Chen, Dinghao Wu


+ [ Towards Stable Backdoor Purification through Feature Shift Tuning](https://arxiv.org//abs/2310.01875)

	Rui Min, Zeyu Qin, Li Shen, Minhao Cheng


+ [ Defending Against Authorship Identification Attacks](https://arxiv.org//abs/2310.01568)

	Haining Wang


+ [ Can Language Models be Instructed to Protect Personal Information?](https://arxiv.org//abs/2310.02224)

	Yang Chen, Ethan Mendes, Sauvik Das, Wei Xu, Alan Ritter


+ [ Adversarial Client Detection via Non-parametric Subspace Monitoring in  the Internet of Federated Things](https://arxiv.org//abs/2310.01537)

	Xianjian Xie, Xiaochen Xian, Dan Li, Andi Wang


+ [ Fool Your (Vision and) Language Model With Embarrassingly Simple  Permutations](https://arxiv.org//abs/2310.01651)

	Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika Chavhan, Timothy Hospedales


+ [ Beyond Labeling Oracles: What does it mean to steal ML models?](https://arxiv.org//abs/2310.01959)

	Avital Shafran, Ilia Shumailov, Murat A. Erdogdu, Nicolas Papernot


+ [ FLEDGE: Ledger-based Federated Learning Resilient to Inference and  Backdoor Attacks](https://arxiv.org//abs/2310.02113)

	Jorge Castillo, Phillip Rieger, Hossein Fereidooni, Qian Chen, Ahmad Sadeghi


+ [ Waveform Manipulation Against DNN-based Modulation Classification  Attacks](https://arxiv.org//abs/2310.01894)

	Dimitrios Varkatzas, Antonios Argyriou


## 2023-10-03
+ [ Adversarial Driving Behavior Generation Incorporating Human Risk  Cognition for Autonomous Vehicle Evaluation](https://arxiv.org//abs/2310.00029)

	Zhen Liu, Hang Gao, Hao Ma, Shuo Cai, Yunfeng Hu, Ting Qu, Hong Chen, Xun Gong


+ [ Certified Robustness via Dynamic Margin Maximization and Improved  Lipschitz Regularization](https://arxiv.org//abs/2310.00116)

	Mahyar Fazlyab, Taha Entesari, Aniket Roy, Rama Chellappa


+ [ Beyond Random Noise: Insights on Anonymization Strategies from a Latent  Bandit Study](https://arxiv.org//abs/2310.00221)

	Alexander Galozy, Sadi Alawadi, Victor Kebande, Sławomir Nowaczyk


+ [ Understanding the Robustness of Randomized Feature Defense Against  Query-Based Adversarial Attacks](https://arxiv.org//abs/2310.00567)

	Quang H. Nguyen, Yingjie Lao, Tung Pham, Kok-Seng Wong, Khoa D. Doan


+ [ Faithful Explanations of Black-box NLP Models Using LLM-generated  Counterfactuals](https://arxiv.org//abs/2310.00603)

	Yair Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma, Roi Reichart


+ [ A Survey of Robustness and Safety of 2D and 3D Deep Learning Models  Against Adversarial Attacks](https://arxiv.org//abs/2310.00633)

	Yanjie Li, Bin Xie, Songtao Guo, Yuanyuan Yang, Bin Xiao



+ [ All Languages Matter: On the Multilingual Safety of Large Language  Models](https://arxiv.org//abs/2310.00905)

	Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, Michael R. Lyu


+ [ Large Language Model-Powered Smart Contract Vulnerability Detection: New  Perspectives](https://arxiv.org//abs/2310.01152)

	Sihao Hu, Tiansheng Huang, Fatih İlhan, Selim Fukan Tekin, Ling Liu



+ [ Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language  Models](https://arxiv.org//abs/2310.00322)

	Chengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan, Yaodong Yang


+ [ Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning](https://arxiv.org//abs/2310.00648)

	Lauren Hong, Ting Wang


+ [ Robustness of AI-Image Detectors: Fundamental Limits and Practical  Attacks](https://arxiv.org//abs/2310.00076)

	Mehrdad Saberi, Vinu Sankar Sadasivan, Keivan Rezaei, Aounon Kumar, Atoosa Chegini, Wenxiao Wang, Soheil Feizi


+ [ Human-Producible Adversarial Examples](https://arxiv.org//abs/2310.00438)

	David Khachaturov, Yue Gao, Ilia Shumailov, Robert Mullins, Ross Anderson, Kassem Fawaz


+ [ Black-box Attacks on Image Activity Prediction and its Natural Language  Explanations](https://arxiv.org//abs/2310.00503)

	Alina Elena Baia, Valentina Poggioni, Andrea Cavallaro


+ [ GhostEncoder: Stealthy Backdoor Attacks with Dynamic Triggers to  Pre-trained Encoders in Self-supervised Learning](https://arxiv.org//abs/2310.00626)

	Qiannan Wang, Changchun Yin, Zhe Liu, Liming Fang, Run Wang, Chenhao Lin


+ [ Counterfactual Image Generation for adversarially robust and  interpretable Classifiers](https://arxiv.org//abs/2310.00761)

	Rafael Bischof, Florian Scheidegger, Michael A. Kraus, A. Cristiano I. Malossi


+ [ Practical Membership Inference Attacks Against Large-Scale Multi-Modal  Models: A Pilot Study](https://arxiv.org//abs/2310.00108)

	Myeongseob Ko, Ming Jin, Chenguang Wang, Ruoxi Jia


+ [ Understanding Adversarial Transferability in Federated Learning](https://arxiv.org//abs/2310.00616)

	Yijiang Li, Ying Gao, Haohan Wang


+ [ On the Onset of Robust Overfitting in Adversarial Training](https://arxiv.org//abs/2310.00607)

	Chaojian Yu, Xiaolong Shi, Jun Yu, Bo Han, Tongliang Liu


+ [ Balancing Efficiency vs. Effectiveness and Providing Missing Label  Robustness in Multi-Label Stream Classification](https://arxiv.org//abs/2310.00665)

	Sepehr Bakhshi, Fazli Can


+ [ Adversarial Explainability: Utilizing Explainable Machine Learning in  Bypassing IoT Botnet Detection Systems](https://arxiv.org//abs/2310.00070)

	Mohammed M. Alani, Atefeh Mashatan, Ali Miri


+ [ Source Inference Attacks: Beyond Membership Inference Attacks in  Federated Learning](https://arxiv.org//abs/2310.00222)

	Hongsheng Hu, Xuyun Zhang, Zoran Salcic, Lichao Sun, Kim-Kwang Raymond Choo, Gillian Dobbie


## 2023-10-02
+ [ AIR: Threats of Adversarial Attacks on Deep Learning-Based Information  Recovery](https://arxiv.org//abs/2309.16706)

	Jinyin Chen, Jie Ge, Shilian Zheng, Linhui Ye, Haibin Zheng, Weiguo Shen, Keqiang Yue, Xiaoniu Yang


+ [ General Lipschitz: Certified Robustness Against Resolvable Semantic  Transformations via Transformation-Dependent Randomized Smoothing](https://arxiv.org//abs/2309.16710)

	Dmitrii Korzh, Mikhail Pautov, Olga Tsymboi, Ivan Oseledets


+ [ Investigating Human-Identifiable Features Hidden in Adversarial  Perturbations](https://arxiv.org//abs/2309.16878)

	Dennis Y. Menn, Tzu-hsun Feng, Sriram Vishwanath, Hung-yi Lee


+ [ Medical Foundation Models are Susceptible to Targeted Misinformation  Attacks](https://arxiv.org//abs/2309.17007)

	Tianyu Han, Sven Nebelung, Firas Khader, Tianci Wang, Gustav Mueller-Franzes, Christiane Kuhl, Sebastian Försch, Jens Kleesiek, Christoph Haarburger, Keno K. Bressem, Jakob Nikolas Kather, Daniel Truhn



+ [ Adversarial Machine Learning in Latent Representations of Neural  Networks](https://arxiv.org//abs/2309.17401)

	Milin Zhang, Mohammad Abdi, Francesco Restuccia


+ [ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending  Against Extraction Attacks](https://arxiv.org//abs/2309.17410)

	Vaidehi Patil, Peter Hase, Mohit Bansal


+ [ LatticeGen: A Cooperative Framework which Hides Generated Text in a  Lattice for Privacy-Aware Generation on Cloud](https://arxiv.org//abs/2309.17157)

	Mengke Zhang, Tianxing He, Tianle Wang, Fatemehsadat Mireshghallah, Binyi Chen, Hao Wang, Yulia Tsvetkov


+ [ Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty  and Smoothness](https://arxiv.org//abs/2309.16973)

	Xiaoyu Wen, Xudong Yu, Rui Yang, Chenjia Bai, Zhen Wang


+ [ Efficient Biologically Plausible Adversarial Training](https://arxiv.org//abs/2309.17348)

	Matilde Tristany Farinha, Thomas Ortner, Giorgia Dellaferrera, Benjamin Grewe, Angeliki Pantazi


+ [ Adversarial Imitation Learning from Visual Observations using Latent  Information](https://arxiv.org//abs/2309.17371)

	Vittorio Giammarino, James Queeney, Ioannis Ch. Paschalidis


## 2023-10-01
+ [ Towards Efficient and Trustworthy AI Through  Hardware-Algorithm-Communication Co-Design](https://arxiv.org//abs/2309.15942)

	Bipin Rajendran, Osvaldo Simeone, Bashir M. Al-Hashimi


+ [ VDC: Versatile Data Cleanser for Detecting Dirty Samples via  Visual-Linguistic Inconsistency](https://arxiv.org//abs/2309.16211)

	Zihao Zhu, Mingda Zhang, Shaokui Wei, Bingzhe Wu, Baoyuan Wu


## 2023-09-30


+ [ Recent Advances of Differential Privacy in Centralized Deep Learning: A  Systematic Survey](https://arxiv.org//abs/2309.16398)

	Lea Demelius, Roman Kern, Andreas Trügler


+ [ Robust Offline Reinforcement Learning -- Certify the Confidence Interval](https://arxiv.org//abs/2309.16631)

	Jiarui Yao, Simon Shaolei Du


## 2023-09-29
+ [ Adversarial Examples Might be Avoidable: The Role of Data Concentration  in Adversarial Robustness](https://arxiv.org//abs/2309.16096)

	Ambar Pal, Jeremias Sulam, René Vidal


+ [ Parameter-Saving Adversarial Training: Reinforcing Multi-Perturbation  Robustness via Hypernetworks](https://arxiv.org//abs/2309.16207)

	Huihui Gong, Minjing Dong, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu


+ [ On the Trade-offs between Adversarial Robustness and Actionable  Explanations](https://arxiv.org//abs/2309.16452)

	Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju


+ [ Resisting Backdoor Attacks in Federated Learning via Bidirectional  Elections and Individual Perspective](https://arxiv.org//abs/2309.16456)

	Zhen Qin, Feiyi Chen, Chen Zhi, Xueqiang Yan, Shuiguang Deng


+ [ Towards Poisoning Fair Representations](https://arxiv.org//abs/2309.16487)

	Tianci Liu, Haoyu Wang, Feijie Wu, Hengtong Zhang, Pan Li, Lu Su, Jing Gao


+ [ Compilation as a Defense: Enhancing DL Model Attack Robustness via  Tensor Optimization](https://arxiv.org//abs/2309.16577)

	Stefan Trawicki, William Hackett, Lewis Birch, Neeraj Suri, Peter Garraghan


+ [ Cyber Sentinel: Exploring Conversational Agents in Streamlining Security  Tasks with GPT-4](https://arxiv.org//abs/2309.16422)

	Mehrdad Kaheh, Danial Khosh Kholgh, Panos Kostakos

## 2023-09-28

+ [ How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking  Unrelated Questions](https://arxiv.org//abs/2309.15840)

	Lorenzo Pacchiardi, Alex J. Chan, Sören Mindermann, Ilan Moscovitz, Alexa Y. Pan, Yarin Gal, Owain Evans, Jan Brauner


+ [ The Robust Semantic Segmentation UNCV2023 Challenge Results](https://arxiv.org//abs/2309.15478)

	Xuanlong Yu, Yi Zuo, Zitao Wang, Xiaowen Zhang, Jiaxuan Zhao, Yuting Yang, Licheng Jiao, Rui Peng, Xinyi Wang, Junpei Zhang, Kexin Zhang, Fang Liu, Roberto Alcover-Couso, Juan C. SanMiguel, Marcos Escudero-Viñolo, Hanlin Tian, Kenta Matsui, Tianhao Wang, Fahmy Adan, Zhitong Gao, Xuming He, Quentin Bouniot, Hossein Moghaddam, Shyam Nandan Rai, Fabio Cermelli, Carlo Masone, Andrea Pilzer, Elisa Ricci, Andrei Bursuc, Arno Solin, Martin Trapp, Rui Li, Angela Yao, Wenlong Chen, Ivor Simpson, Neill D. F. Campbell, Gianni Franchi


+ [ A Unified View of Differentially Private Deep Generative Modeling](https://arxiv.org//abs/2309.15696)

	Dingfan Chen, Raouf Kerkouche, Mario Fritz


+ [ On Computational Entanglement and Its Interpretation in Adversarial  Machine Learning](https://arxiv.org//abs/2309.15669)

	YenLung Lai, Xingbo Dong, Zhe Jin


+ [ Automatic Feature Fairness in Recommendation via Adversaries](https://arxiv.org//abs/2309.15418)

	Hengchang Hu, Yiming Cao, Zhankui He, Samson Tan, Min-Yen Kan


## 2023-09-27
+ [ Bias Assessment and Mitigation in LLM-based Code Generation](https://arxiv.org//abs/2309.14345)

	Dong Huang, Qingwen Bu, Jie Zhang, Xiaofei Xie, Junjie Chen, Heming Cui


+ [ Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM](https://arxiv.org//abs/2309.14348)

	Bochuan Cao, Yuanpu Cao, Lu Lin, Jinghui Chen


+ [ Survey of Social Bias in Vision-Language Models](https://arxiv.org//abs/2309.14381)

	Nayeon Lee, Yejin Bang, Holy Lovenia, Samuel Cahyawijaya, Wenliang Dai, Pascale Fung


+ [ XGV-BERT: Leveraging Contextualized Language Model and Graph Neural  Network for Efficient Software Vulnerability Detection](https://arxiv.org//abs/2309.14677)

	Vu Le Anh Quan, Chau Thuan Phat, Kiet Van Nguyen, Phan The Duy, Van-Hau Pham


+ [ DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature  Space](https://arxiv.org//abs/2309.14585)

	Liu Jun, Zhou Jiantao, Zeng Jiandian, Jinyu Tian


+ [ Structure Invariant Transformation for better Adversarial  Transferability](https://arxiv.org//abs/2309.14700)

	Xiaosen Wang, Zeliang Zhang, Jianping Zhang


+ [ Frugal Satellite Image Change Detection with Deep-Net Inversion](https://arxiv.org//abs/2309.14781)

	Hichem Sahbi, Sebastien Deschamps


+ [ The Surveillance AI Pipeline](https://arxiv.org//abs/2309.15084)

	Pratyusha Ria Kalluri, William Agnew, Myra Cheng, Kentrell Owens, Luca Soldaini, Abeba Birhane


+ [ Unveiling Fairness Biases in Deep Learning-Based Brain MRI  Reconstruction](https://arxiv.org//abs/2309.14392)

	Yuning Du, Yuyang Xue, Rohan Dharmakumar, Sotirios A. Tsaftaris


+ [ LogGPT: Log Anomaly Detection via GPT](https://arxiv.org//abs/2309.14482)

	Xiao Han, Shuhan Yuan, Mohamed Trabelsi


+ [ Privacy-preserving and Privacy-attacking Approaches for Speech and Audio  -- A Survey](https://arxiv.org//abs/2309.15087)

	Yuchen Liu, Apu Kapadia, Donald Williamson

## 2023-09-26
+ [ Investigating Efficient Deep Learning Architectures For Side-Channel  Attacks on AES](https://arxiv.org//abs/2309.13170)

	Yohaï-Eliel Berreby, Laurent Sauvage


+ [ Towards Green AI in Fine-tuning Large Language Models via Adaptive  Backpropagation](https://arxiv.org//abs/2309.13192)

	Kai Huang, Hanyun Yin, Heng Huang, Wei Gao


+ [ Defending Pre-trained Language Models as Few-shot Learners against  Backdoor Attacks](https://arxiv.org//abs/2309.13256)

	Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, Ting Wang


+ [ LLMs as Counterfactual Explanation Modules: Can ChatGPT Explain  Black-box Text Classifiers?](https://arxiv.org//abs/2309.13340)

	Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, Huan Liu


+ [ Seeing Is Not Always Believing: Invisible Collision Attack and Defence  on Pre-Trained Models](https://arxiv.org//abs/2309.13579)

	Minghang Deng, Zhong Zhang, Junming Shao


+ [ PRIS: Practical robust invertible network for image steganography](https://arxiv.org//abs/2309.13620)

	Hang Yang, Yitian Xu, Xuhua Liu, Xiaodong Ma


+ [ GHN-QAT: Training Graph Hypernetworks to Predict Quantization-Robust  Parameters of Unseen Limited Precision Neural Networks](https://arxiv.org//abs/2309.13773)

	Stone Yun, Alexander Wong


+ [ Can LLM-Generated Misinformation Be Detected?](https://arxiv.org//abs/2309.13788)

	Canyu Chen, Kai Shu


+ [ RBFormer: Improve Adversarial Robustness of Transformer by Robust Bias](https://arxiv.org//abs/2309.13245)

	Hao Cheng, Jinhao Duan, Hui Li, Lyutianyang Zhang, Jiahang Cao, Ping Wang, Jize Zhang, Kaidi Xu, Renjing Xu


+ [ DFRD: Data-Free Robustness Distillation for Heterogeneous Federated  Learning](https://arxiv.org//abs/2309.13546)

	Kangyang Luo, Shuai Wang, Yexuan Fu, Xiang Li, Yunshi Lan, Ming Gao


+ [ Vulnerabilities in Video Quality Assessment Models: The Challenge of  Adversarial Attacks](https://arxiv.org//abs/2309.13609)

	Ao-Xiang Zhang, Yu Ran, Weixuan Tang, Yuan-Gen Wang


+ [ Video Adverse-Weather-Component Suppression Network via Weather  Messenger and Adversarial Backpropagation](https://arxiv.org//abs/2309.13700)

	Yijun Yang, Angelica I. Aviles-Rivero, Huazhu Fu, Ye Liu, Weiming Wang, Lei Zhu


+ [ Combining Two Adversarial Attacks Against Person Re-Identification  Systems](https://arxiv.org//abs/2309.13763)

	Eduardo de O. Andrade, Igor Garcia Ballhausen Sampaio, Joris Guérin, José Viterbo


+ [ Adversarial Attacks on Video Object Segmentation with Hard Region  Discovery](https://arxiv.org//abs/2309.13857)

	Ping Li, Yu Zhang, Li Yuan, Jian Zhao, Xianghua Xu, Xiaoqin Zhang


+ [ SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via  Substitution](https://arxiv.org//abs/2309.14122)

	Zhongjie Ba, Jieming Zhong, Jiachen Lei, Peng Cheng, Qinglong Wang, Zhan Qin, Zhibo Wang, Kui Ren



+ [ Spatial-frequency channels, shape bias, and adversarial robustness](https://arxiv.org//abs/2309.13190)

	Ajay Subramanian, Elena Sizikova, Najib J. Majaj, Denis G. Pelli


+ [ Beyond Fairness: Age-Harmless Parkinson's Detection via Voice](https://arxiv.org//abs/2309.13292)

	Yicheng Wang, Xiaotian Han, Leisheng Yu, Na Zou


+ [ Improving Robustness of Deep Convolutional Neural Networks via  Multiresolution Learning](https://arxiv.org//abs/2309.13752)

	Hongyan Zhou, Yao Liang


+ [ Invisible Watermarking for Audio Generation Diffusion Models](https://arxiv.org//abs/2309.13166)

	Xirong Cao, Xiang Li, Divyesh Jadav, Yanzhao Wu, Zhehui Chen, Chen Zeng, Wenqi Wei


+ [ On the Effectiveness of Adversarial Samples against Ensemble  Learning-based Windows PE Malware Detectors](https://arxiv.org//abs/2309.13841)

	Trong-Nghia To, Danh Le Kim, Do Thi Thu Hien, Nghi Hoang Khoa, Hien Do Hoang, Phan The Duy, Van-Hau Pham


## 2023-09-25
+ [ Provably Robust and Plausible Counterfactual Explanations for Neural  Networks via Robust Optimisation](https://arxiv.org//abs/2309.12545)

	Junqi Jiang, Jianglin Lan, Francesco Leofante, Antonio Rago, Francesca Toni


+ [ HANS, are you clever? Clever Hans Effect Analysis of Neural Systems](https://arxiv.org//abs/2309.12481)

	Leonardo Ranaldi, Fabio Massimo Zanzotto


+ [ Privacy Assessment on Reconstructed Images: Are Existing Evaluation  Metrics Faithful to Human Perception?](https://arxiv.org//abs/2309.13038)

	Xiaoxiao Sun, Nidham Gazagnadou, Vivek Sharma, Lingjuan Lyu, Hongdong Li, Liang Zheng


+ [ Improving Machine Learning Robustness via Adversarial Training](https://arxiv.org//abs/2309.12593)

	Long Dang, Thushari Hapuarachchi, Kaiqi Xiong, Jing Lin


+ [ On Data Fabrication in Collaborative Vehicular Perception: Attacks and  Countermeasures](https://arxiv.org//abs/2309.12955)

	Qingzhao Zhang, Shuowei Jin, Jiachen Sun, Xumiao Zhang, Ruiyang Zhu, Qi Alfred Chen, Z. Morley Mao


+ [ Robotic Handling of Compliant Food Objects by Robust Learning from  Demonstration](https://arxiv.org//abs/2309.12856)

	Ekrem Misimi, Alexander Olofsson, Aleksander Eilertsen, Elling Ruud Øye, John Reidar Mathiassen


## 2023-09-24
+ [ Distilling Adversarial Prompts from Safety Benchmarks: Report for the  Adversarial Nibbler Challenge](https://arxiv.org//abs/2309.11575)

	Manuel Brack, Patrick Schramowski, Kristian Kersting


+ [ The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"](https://arxiv.org//abs/2309.12288)

	Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans


## 2023-09-23
+ [ Bad Actor, Good Advisor: Exploring the Role of Large Language Models in  Fake News Detection](https://arxiv.org//abs/2309.12247)

	Beizhe Hu, Qiang Sheng, Juan Cao, Yuhui Shi, Yang Li, Danding Wang, Peng Qi


+ [ A Chinese Prompt Attack Dataset for LLMs with Evil Content](https://arxiv.org//abs/2309.11830)

	Chengyuan Liu, Fubang Zhao, Lizhi Qing, Yangyang Kang, Changlong Sun, Kun Kuang, Fei Wu


+ [ Vulnerability of 3D Face Recognition Systems to Morphing Attacks](https://arxiv.org//abs/2309.12118)

	Sanjeet Vardam, Luuk Spreeuwers


+ [ Towards Differential Privacy in Sequential Recommendation: A Noisy Graph  Neural Network Approach](https://arxiv.org//abs/2309.11515)

	Wentao Hu, Hui Fang


## 2023-09-22
+ [ CATS: Conditional Adversarial Trajectory Synthesis for  Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches](https://arxiv.org//abs/2309.11587)

	Jinmeng Rao, Song Gao, Sijia Zhu


+ [ How Robust is Google's Bard to Adversarial Image Attacks?](https://arxiv.org//abs/2309.11751)

	Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, Jun Zhu


+ [ Knowledge Sanitization of Large Language Models](https://arxiv.org//abs/2309.11852)

	Yoichi Ishibashi, Hidetoshi Shimodaira


+ [ On the Relationship between Skill Neurons and Robustness in Prompt  Tuning](https://arxiv.org//abs/2309.12263)

	Leon Ackermann, Xenia Ohmer


+ [ TextCLIP: Text-Guided Face Image Generation And Manipulation Without  Adversarial Training](https://arxiv.org//abs/2309.11923)

	Xiaozhou You, Jian Zhang


+ [ Dictionary Attack on IMU-based Gait Authentication](https://arxiv.org//abs/2309.11766)

	Rajesh Kumar, Can Isik, Chilukuri K. Mohan
  

+ [ Privacy-Preserving In-Context Learning with Differentially Private  Few-Shot Generation](https://arxiv.org//abs/2309.11765)

	Xinyu Tang, Richard Shin, Huseyin A. Inan, Andre Manoel, Fatemehsadat Mireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Robert Sim


+ [ MarkNerf:Watermarking for Neural Radiance Field](https://arxiv.org//abs/2309.11747)

	Lifeng Chen, Jia Liu, Yan Ke, Wenquan Sun, Weina Dong, Xiaozhong Pan


+ [ DeepTheft: Stealing DNN Model Architectures through Power Side Channel](https://arxiv.org//abs/2309.11894)

	Yansong Gao, Huming Qiu, Zhi Zhang, Binghui Wang, Hua Ma, Alsharif Abuadbba, Minhui Xue, Anmin Fu, Surya Nepal


## 2023-09-21
+ [ When to Trust AI: Advances and Challenges for Certification of Neural  Networks](https://arxiv.org//abs/2309.11196)

	Marta Kwiatkowska, Xiyue Zhang


+ [ C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for  Physics-based Characters](https://arxiv.org//abs/2309.11351)

	Zhiyang Dou, Xuelin Chen, Qingnan Fan, Taku Komura, Wenping Wang


+ [ What Learned Representations and Influence Functions Can Tell Us About  Adversarial Examples](https://arxiv.org//abs/2309.10916)

	Shakila Mahjabin Tonni, Mark Dras


+ [ PRAT: PRofiling Adversarial aTtacks](https://arxiv.org//abs/2309.11111)

	Rahul Ambati, Naveed Akhtar, Ajmal Mian, Yogesh Singh Rawat


+ [ It's Simplex! Disaggregating Measures to Improve Certified Robustness](https://arxiv.org//abs/2309.11005)

	Andrew C. Cullen, Paul Montague, Shijie Liu, Sarah M. Erfani, Benjamin I.P. Rubinstein


+ [ Learning Patient Static Information from Time-series EHR and an Approach  for Safeguarding Privacy and Fairness](https://arxiv.org//abs/2309.11373)

	Wei Liao, Joel Voldman


+ [ Fed-LSAE: Thwarting Poisoning Attacks against Federated Cyber Threat  Detection System via Autoencoder-based Latent Space Inspection](https://arxiv.org//abs/2309.11053)

	Tran Duc Luong, Vuong Minh Tien, Nguyen Huu Quyen, Do Thi Thu Hien, Phan The Duy, Van-Hau Pham


## 2023-09-20
+ [ GPTFUZZER : Red Teaming Large Language Models with Auto-Generated  Jailbreak Prompts](https://arxiv.org//abs/2309.10253) 

  Jiahao Yu, Xingwei Lin, Xinyu Xing


+ [ Exploring the Dark Side of AI: Advanced Phishing Attack Design and  Deployment Using ChatGPT](https://arxiv.org//abs/2309.10463) 

  Nils Begou, Jeremy Vinoy, Andrzej Duda, Maciej Korczynski


+ [ Transferable Adversarial Attack on Image Tampering Localization](https://arxiv.org//abs/2309.10243)

  Yuqi Wang, Gang Cao, Zijie Lou, Haochen Zhu


+ [ RECALL+: Adversarial Web-based Replay for Continual Learning in Semantic  Segmentation](https://arxiv.org//abs/2309.10479)

  Chang Liu, Giulia Rizzoli, Francesco Barbato, Umberto Michieli, Yi Niu, Pietro Zanuttigh


+ [ Realistic Website Fingerprinting By Augmenting Network Trace](https://arxiv.org//abs/2309.10147)

  Alireza Bahramali, Ardavan Bozorgi, Amir Houmansadr


+ [ Love or Hate? Share or Split? Privacy-Preserving Training Using Split  Learning and Homomorphic Encryption](https://arxiv.org//abs/2309.10517) 

  Tanveer Khan, Khoa Nguyen, Antonis Michalas, Alexandros Bakas


+ [ Disentangled Information Bottleneck guided Privacy-Protective JSCC for  Image Transmission](https://arxiv.org//abs/2309.10263)

  Lunan Sun, Yang Yang, Mingzhe Chen, Caili Guo


+ [ SPFL: A Self-purified Federated Learning Method Against Poisoning  Attacks](https://arxiv.org//abs/2309.10607) 

  Zizhen Liu, Weiyang He, Chip-Hong Chang, Jing Ye, Huawei Li, Xiaowei Li


## 2023-09-19
+ [ Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents](https://arxiv.org//abs/2309.09919)

  Ziyi Yang, Shreyas S. Raman, Ankit Shah, Stefanie Tellex


+ [ Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract  Code Using Vulnerability-constrained Decoding](https://arxiv.org//abs/2309.09826)

  André Storhaug, Jingyue Li, Tianyuan Hu


+ [ Bias of AI-Generated Content: An Examination of News Produced by Large  Language Models](https://arxiv.org//abs/2309.09825)

  Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe Zhang, Ming Zhao, Xiaohang Zhao


+ [ Reducing Adversarial Training Cost with Gradient Approximation](https://arxiv.org//abs/2309.09464)

  Huihui Gong, Shuo Yang, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu


+ [ Stealthy Physical Masked Face Recognition Attack via Adversarial Style  Optimization](https://arxiv.org//abs/2309.09480)

  Huihui Gong, Minjing Dong, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu


+ [ Evaluating Adversarial Robustness with Expected Viable Performance](https://arxiv.org//abs/2309.09928)

  Ryan McCoppin, Colin Dawson, Sean M. Kennedy, Leslie M. Blaha

## 2023-09-17

+ [ Convex Latent-Optimized Adversarial Regularizers for Imaging Inverse  Problems](https://arxiv.org//abs/2309.09250)

  Huayu Wang, Chen Luo, Taofeng Xie, Qiyu Jin, Guoqing Chen, Zhuo-Xu Cui, Dong Liang

## 2023-09-16


+ [ Robust Backdoor Attacks on Object Detection in Real World](https://arxiv.org//abs/2309.08953)

  Yaguan Qian, Boyuan Ji, Shuke He, Shenhui Huang, Xiang Ling, Bin Wang, Wei Wang


+ [ Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and  Nationality Bias in Generative Models](https://arxiv.org//abs/2309.08902)

  Mahammed Kamruzzaman, Md. Minul Islam Shovon, Gene Louis Kim
 

+ [ Context-aware Adversarial Attack on Named Entity Recognition](https://arxiv.org//abs/2309.08999)

  Shuguang Chen, Leonardo Neves, Thamar Solorio
 
## 2023-09-15

+ [ Adversarial Attacks on Tables with Entity Swap](https://arxiv.org//abs/2309.08650)

  Aneta Koleva, Martin Ringsquandl, Volker Tresp


+ [ A More Secure Split: Enhancing the Security of Privacy-Preserving Split  Learning](https://arxiv.org//abs/2309.08697)

  Tanveer Khan, Khoa Nguyen, Antonis Michalas

  
+ [ Detecting Unknown Attacks in IoT Environments: An Open Set Classifier  for Enhanced Network Intrusion Detection](https://arxiv.org//abs/2309.07461)

  Yasir Ali Farrukh, Syed Wali, Irfan Khan, Nathaniel D. Bastian


+ [ Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated  Text](https://arxiv.org//abs/2309.07689)

  Mahdi Dhaini, Wessel Poelman, Ege Erdogan


+ [ Keep your Identity Small: Privacy-preserving Client-side Fingerprinting](https://arxiv.org//abs/2309.07563)

  Alberto Fernandez-de-Retana, Igor Santos-Grueiro


+ [ Fake News Detectors are Biased against Texts Generated by Large Language  Models](https://arxiv.org//abs/2309.08674)

  Jinyan Su, Terry Yue Zhuo, Jonibek Mansurov, Di Wang, Preslav Nakov
