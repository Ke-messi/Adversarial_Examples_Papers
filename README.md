# A compelete list of papers about adversarial examples

It appears that the [List of All Adversarial Example Papers](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html) has been experiencing crashes over the past few days. In the absence of this valuable resource, staying up-to-date with the latest research papers in this field has become challenging. Consequently, I created a repository aimed at aggregating and maintaining the most current papers in this domain. While this repository may not encompass every paper, I did try. If you find any papers we have missed, just drop me an [email](mailto:xswanghuster@gmail.com). We have included the [data](./nicholas.md) from [List of All Adversarial Example Papers](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html) till 2023-09-01.
## 2023-10-10
+ [ Fingerprint Attack: Client De-Anonymization in Federated Learning](https://arxiv.org//abs/2310.05960)`uncheck`

	Qiongkai Xu, Trevor Cohn, Olga Ohrimenko


+ [ Suppressing Overestimation in Q-Learning through Adversarial Behaviors](https://arxiv.org//abs/2310.06286)`uncheck`

	HyeAnn Lee, Donghwan Lee


+ [ Jailbreak and Guard Aligned Language Models with Only Few In-Context  Demonstrations](https://arxiv.org//abs/2310.06387)`uncheck`

	Zeming Wei, Yifei Wang, Yisen Wang


+ [ Multilingual Jailbreak Challenges in Large Language Models](https://arxiv.org//abs/2310.06474)`uncheck`

	Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing


+ [ A Semantic Invariant Robust Watermark for Large Language Models](https://arxiv.org//abs/2310.06356)`uncheck`

	Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, Lijie Wen


+ [ Adversarial Masked Image Inpainting for Robust Detection of Mpox and  Non-Mpox](https://arxiv.org//abs/2310.06318)`uncheck`

	Yubiao Yue, Zhenzhang Li


+ [ Leveraging Diffusion-Based Image Variations for Robust Training on  Poisoned Data](https://arxiv.org//abs/2310.06372)`uncheck`

	Lukas Struppek, Martin B. Hentschel, Clifton Poth, Dominik Hintersdorf, Kristian Kersting


+ [ Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield  but Also a Catalyst for Model Inversion Attacks](https://arxiv.org//abs/2310.06549)`uncheck`

	Lukas Struppek, Dominik Hintersdorf, Kristian Kersting


+ [ Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK  Approach](https://arxiv.org//abs/2310.06112)`uncheck`

	Shaopeng Fu, Di Wang


+ [ PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust  Generalization](https://arxiv.org//abs/2310.06182)`uncheck`

	Jiancong Xiao, Ruoyu Sun, Zhi-quan Luo


+ [ Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach](https://arxiv.org//abs/2310.06396)`uncheck`

	Kai Zhao, Qiyu Kang, Yang Song, Rui She, Sijie Wang, Wee Peng Tay


+ [ Exploring adversarial attacks in federated learning for medical imaging](https://arxiv.org//abs/2310.06227)`uncheck`

	Erfan Darzi, Florian Dubost, N.M. Sijtsema, P.M.A van Ooijen


## 2023-10-09
+ [ Robust Network Pruning With Sparse Entropic Wasserstein Regression](https://arxiv.org//abs/2310.04918)`uncheck`

	Lei You, Hei Victor Cheng


+ [ The Troubling Emergence of Hallucination in Large Language Models -- An  Extensive Definition, Quantification, and Prescriptive Remediations](https://arxiv.org//abs/2310.04988)`uncheck`

	Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das


+ [ Large Language Models Can Be Good Privacy Protection Learners](https://arxiv.org//abs/2310.02469)`uncheck`

	Yijia Xiao, Yiqiao Jin, Yushi Bai, Yue Wu, Xianjun Yang, Xiao Luo, Wenchao Yu, Xujiang Zhao, Yanchi Liu, Haifeng Chen, Wei Wang, Wei Cheng


+ [ LoFT: Local Proxy Fine-tuning For Improving Transferability Of  Adversarial Attacks Against Large Language Model](https://arxiv.org//abs/2310.04445)`uncheck`

	Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, Raphael Olivier, Ankit Shah, Dareen Alharthi, Hazim T Bukhari, Massa Baali, Soham Deshmukh, Michael Kuhlmann, Bhiksha Raj, Rita Singh


+ [ AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language  Models](https://arxiv.org//abs/2310.04451)`uncheck`

	Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao


+ [ Can pruning make Large Language Models more efficient?](https://arxiv.org//abs/2310.04573)`uncheck`

	Sia Gholami, Marwan Omar


+ [ Understanding and Improving Adversarial Attacks on Latent Diffusion  Model](https://arxiv.org//abs/2310.04687)`uncheck`

	Boyang Zheng, Chumeng Liang, Xiaoyu Wu, Yan Liu


+ [ Robustness-enhanced Uplift Modeling with Adversarial Feature  Desensitization](https://arxiv.org//abs/2310.04693)`uncheck`

	Zexu Sun, Bowei He, Ming Ma, Jiakai Tang, Yuchen Wang, Chen Ma, Dugang Liu


+ [ Are Large Language Models Post Hoc Explainers?](https://arxiv.org//abs/2310.05797)`uncheck`

	Nicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju


+ [ Better Safe than Sorry: Pre-training CLIP against Targeted Data  Poisoning and Backdoor Attacks](https://arxiv.org//abs/2310.05862)`uncheck`

	Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman


+ [ Self-Knowledge Guided Retrieval Augmentation for Large Language Models](https://arxiv.org//abs/2310.05002)`uncheck`

	Yile Wang, Peng Li, Maosong Sun, Yang Liu


+ [ BRAINTEASER: Lateral Thinking Puzzles for Large Language Model](https://arxiv.org//abs/2310.05057)`uncheck`

	Yifan Jiang, Filip Ilievski, Kaixin Ma


+ [ Do Large Language Models Know about Facts?](https://arxiv.org//abs/2310.05177)`uncheck`

	Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S. Yu, Zhijiang Guo


+ [ SC-Safety: A Multi-round Open-ended Question Adversarial Safety  Benchmark for Large Language Models in Chinese](https://arxiv.org//abs/2310.05818)`uncheck`

	Liang Xu, Kangkang Zhao, Lei Zhu, Hang Xue


+ [ IPMix: Label-Preserving Data Augmentation Method for Training Robust  Classifiers](https://arxiv.org//abs/2310.04780)`uncheck`

	Zhenglin Huang, Xianan Bao, Na Zhang, Qingqi Zhang, Xiaomei Tu, Biao Wu, Xi Yang


+ [ VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via  Pre-trained Models](https://arxiv.org//abs/2310.04655)`uncheck`

	Ziyi Yin, Muchao Ye, Tianrong Zhang, Tianyu Du, Jinguo Zhu, Han Liu, Jinghui Chen, Ting Wang, Fenglong Ma


+ [ GReAT: A Graph Regularized Adversarial Training Method](https://arxiv.org//abs/2310.05336)`uncheck`

	Samet Bayram, Kenneth Barner


+ [ Generating Less Certain Adversarial Examples Improves Robust  Generalization](https://arxiv.org//abs/2310.04539)`uncheck`

	Minxing Zhang, Michael Backes, Xiao Zhang


+ [ Protecting Sensitive Data through Federated Co-Training](https://arxiv.org//abs/2310.05696)`uncheck`

	Amr Abourayya, Jens Kleesiek, Kanishka Rao, Erman Ayday, Bharat Rao, Geoff Webb, Michael Kamp


+ [ Tight Certified Robustness via Min-Max Representations of ReLU Neural  Networks](https://arxiv.org//abs/2310.04916)`uncheck`

	Brendon G. Anderson, Samuel Pfrommer, Somayeh Sojoudi


## 2023-10-08
+ [ Lightweight Boosting Models for User Response Prediction Using  Adversarial Validation](https://arxiv.org//abs/2310.03778)

	Hyeonwoo Kim, Wonsung Lee


+ [ Assessing Robustness via Score-Based Adversarial Image Generation](https://arxiv.org//abs/2310.04285)

	Marcel Kollovieh, Lukas Gosch, Yan Scholten, Marten Lienen, Stephan Günnemann


+ [ Indirect Meltdown: Building Novel Side-Channel Attacks from  Transient-Execution Attacks](https://arxiv.org//abs/2310.04183)

	Daniel Weber, Fabian Thomas, Lukas Gerlach, Ruiyi Zhang, Michael Schwarz


## 2023-10-07
+ [ Benchmarking Local Robustness of High-Accuracy Binary Neural Networks  for Enhanced Traffic Sign Recognition](https://arxiv.org//abs/2310.03033)

	Andreea Postovan, Mădălina Eraşcu
	

+ [ Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models](https://arxiv.org//abs/2310.03123)

	Zihao Lin, Yan Sun, Yifan Shi, Xueqian Wang, Lifu Huang, Li Shen, Dacheng Tao


+ [ Fine-tuning Aligned Language Models Compromises Safety, Even When Users  Do Not Intend To!](https://arxiv.org//abs/2310.03693)

	Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson


+ [ Ask for Alice: Online Covert Distress Signal in the Presence of a Strong  Adversary](https://arxiv.org//abs/2310.03237)

	Hayyu Imanda, Kasper Rasmussen


## 2023-10-06
+ [ Misusing Tools in Large Language Models With Visual Adversarial Examples](https://arxiv.org//abs/2310.03185)

	Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, Earlence Fernandes


+ [ Robust Representation Learning via Asymmetric Negative Contrast and  Reverse Attention](https://arxiv.org//abs/2310.03358)

	Nuoyan Zhou, Decheng Liu, Dawei Zhou, Xinbo Gao, Nannan Wang


+ [ SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org//abs/2310.03684)

	Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas


+ [ A Formalism and Approach for Improving Robustness of Large Language  Models Using Risk-Adjusted Confidence Scores](https://arxiv.org//abs/2310.03283)

	Ke Shen, Mayank Kejriwal


+ [ Shielding the Unseen: Privacy Protection through Poisoning NeRF with  Spatial Deformation](https://arxiv.org//abs/2310.03125)

	Yihan Wu, Brandon Y. Feng, Heng Huang


+ [ CSI: Enhancing the Robustness of 3D Point Cloud Recognition against  Corruption](https://arxiv.org//abs/2310.03360)

	Zhuoyuan Wu, Jiachen Sun, Chaowei Xiao


+ [ OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable  Evasion Attacks](https://arxiv.org//abs/2310.03707)

	Ofir Bar Tal, Adi Haviv, Amit H. Bermano


+ [ Untargeted White-box Adversarial Attack with Heuristic Defence Methods  in Real-time Deep Learning based Network Intrusion Detection System](https://arxiv.org//abs/2310.03334)

	Khushnaseeb Roshan, Aasim Zafar, Sheikh Burhan Ul Haque


+ [ Targeted Adversarial Attacks on Generalizable Neural Radiance Fields](https://arxiv.org//abs/2310.03578)

	Andras Horvath, Csaba M. Jozsa


+ [ Adversarial Machine Learning for Social Good: Reframing the Adversary as  an Ally](https://arxiv.org//abs/2310.03614)

	Shawqi Al-Maliki, Adnan Qayyum, Hassan Ali, Mohamed Abdallah, Junaid Qadir, Dinh Thai Hoang, Dusit Niyato, Ala Al-Fuqaha


+ [ Raze to the Ground: Query-Efficient Adversarial HTML Attacks on  Machine-Learning Phishing Webpage Detectors](https://arxiv.org//abs/2310.03166)

	Biagio Montaruli, Luca Demetrio, Maura Pintor, Luca Compagna, Davide Balzarotti, Battista Biggio


+ [ Regret Analysis of Distributed Online Control for LTI Systems with  Adversarial Disturbances](https://arxiv.org//abs/2310.03206)

	Ting-Jui Chang, Shahin Shahrampour


+ [ Certifiably Robust Graph Contrastive Learning](https://arxiv.org//abs/2310.03312)

	Minhua Lin, Teng Xiao, Enyan Dai, Xiang Zhang, Suhang Wang


+ [ An Integrated Algorithm for Robust and Imperceptible Audio Adversarial  Examples](https://arxiv.org//abs/2310.03349)

	Armin Ettenhofer, Jan-Philipp Schulze, Karla Pizzi


## 2023-10-05
+ [ Low-Resource Languages Jailbreak GPT-4](https://arxiv.org//abs/2310.02446)

	Zheng-Xin Yong, Cristina Menghini, Stephen H. Bach


+ [ Discovering General Reinforcement Learning Algorithms with Adversarial  Environment Design](https://arxiv.org//abs/2310.02782)

	Matthew Thomas Jackson, Minqi Jiang, Jack Parker-Holder, Risto Vuorio, Chris Lu, Gregory Farquhar, Shimon Whiteson, Jakob Nicolaus Foerster


+ [ Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models](https://arxiv.org//abs/2310.02949)

	Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, Dahua Lin


+ [ Can Large Language Models Provide Security & Privacy Advice? Measuring  the Ability of LLMs to Refute Misconceptions](https://arxiv.org//abs/2310.02431)

	Yufan Chen, Arjun Arunasalam, Z. Berkay Celik


+ [ SlowFormer: Universal Adversarial Patch for Attack on Compute and Energy  Efficiency of Inference Efficient Vision Transformers](https://arxiv.org//abs/2310.02544)

	KL Navaneet, Soroush Abbasi Koohpayegani, Essam Sleiman, Hamed Pirsiavash


+ [ Splitting the Difference on Adversarial Training](https://arxiv.org//abs/2310.02480)

	Matan Levi, Aryeh Kontorovich


+ [ A Recipe for Improved Certifiable Robustness: Capacity and Data](https://arxiv.org//abs/2310.02513)

	Kai Hu, Klas Leino, Zifan Wang, Matt Fredrikson


+ [ Jailbreaker in Jail: Moving Target Defense for Large Language Models](https://arxiv.org//abs/2310.02417)

	Bocheng Chen, Advait Paliwal, Qiben Yan


## 2023-10-04
+ [ Identifying and Mitigating Privacy Risks Stemming from Language Models:  A Survey](https://arxiv.org//abs/2310.01424)

	Victoria Smith, Ali Shahin Shamsabadi, Carolyn Ashurst, Adrian Weller


+ [ Fooling the Textual Fooler via Randomizing Latent Representations](https://arxiv.org//abs/2310.01452)

	Duy C. Hoang, Quang H. Nguyen, Saurav Manchanda, MinLong Peng, Kok-Seng Wong, Khoa D. Doan


+ [ On the Safety of Open-Sourced Large Language Models: Does Alignment  Really Prevent Them From Being Misused?](https://arxiv.org//abs/2310.01581)

	Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu Lin, Jinyuan Jia, Jinghui Chen, Dinghao Wu


+ [ Towards Stable Backdoor Purification through Feature Shift Tuning](https://arxiv.org//abs/2310.01875)

	Rui Min, Zeyu Qin, Li Shen, Minhao Cheng


+ [ Defending Against Authorship Identification Attacks](https://arxiv.org//abs/2310.01568)

	Haining Wang


+ [ Can Language Models be Instructed to Protect Personal Information?](https://arxiv.org//abs/2310.02224)

	Yang Chen, Ethan Mendes, Sauvik Das, Wei Xu, Alan Ritter


+ [ Adversarial Client Detection via Non-parametric Subspace Monitoring in  the Internet of Federated Things](https://arxiv.org//abs/2310.01537)

	Xianjian Xie, Xiaochen Xian, Dan Li, Andi Wang


+ [ Fool Your (Vision and) Language Model With Embarrassingly Simple  Permutations](https://arxiv.org//abs/2310.01651)

	Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika Chavhan, Timothy Hospedales


+ [ Beyond Labeling Oracles: What does it mean to steal ML models?](https://arxiv.org//abs/2310.01959)

	Avital Shafran, Ilia Shumailov, Murat A. Erdogdu, Nicolas Papernot


+ [ FLEDGE: Ledger-based Federated Learning Resilient to Inference and  Backdoor Attacks](https://arxiv.org//abs/2310.02113)

	Jorge Castillo, Phillip Rieger, Hossein Fereidooni, Qian Chen, Ahmad Sadeghi


+ [ Waveform Manipulation Against DNN-based Modulation Classification  Attacks](https://arxiv.org//abs/2310.01894)

	Dimitrios Varkatzas, Antonios Argyriou


## 2023-10-03
+ [ Adversarial Driving Behavior Generation Incorporating Human Risk  Cognition for Autonomous Vehicle Evaluation](https://arxiv.org//abs/2310.00029)

	Zhen Liu, Hang Gao, Hao Ma, Shuo Cai, Yunfeng Hu, Ting Qu, Hong Chen, Xun Gong


+ [ Certified Robustness via Dynamic Margin Maximization and Improved  Lipschitz Regularization](https://arxiv.org//abs/2310.00116)

	Mahyar Fazlyab, Taha Entesari, Aniket Roy, Rama Chellappa


+ [ Beyond Random Noise: Insights on Anonymization Strategies from a Latent  Bandit Study](https://arxiv.org//abs/2310.00221)

	Alexander Galozy, Sadi Alawadi, Victor Kebande, Sławomir Nowaczyk


+ [ Understanding the Robustness of Randomized Feature Defense Against  Query-Based Adversarial Attacks](https://arxiv.org//abs/2310.00567)

	Quang H. Nguyen, Yingjie Lao, Tung Pham, Kok-Seng Wong, Khoa D. Doan


+ [ Faithful Explanations of Black-box NLP Models Using LLM-generated  Counterfactuals](https://arxiv.org//abs/2310.00603)

	Yair Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma, Roi Reichart


+ [ A Survey of Robustness and Safety of 2D and 3D Deep Learning Models  Against Adversarial Attacks](https://arxiv.org//abs/2310.00633)

	Yanjie Li, Bin Xie, Songtao Guo, Yuanyuan Yang, Bin Xiao



+ [ All Languages Matter: On the Multilingual Safety of Large Language  Models](https://arxiv.org//abs/2310.00905)

	Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, Michael R. Lyu


+ [ Large Language Model-Powered Smart Contract Vulnerability Detection: New  Perspectives](https://arxiv.org//abs/2310.01152)

	Sihao Hu, Tiansheng Huang, Fatih İlhan, Selim Fukan Tekin, Ling Liu



+ [ Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language  Models](https://arxiv.org//abs/2310.00322)

	Chengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan, Yaodong Yang


+ [ Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning](https://arxiv.org//abs/2310.00648)

	Lauren Hong, Ting Wang


+ [ Robustness of AI-Image Detectors: Fundamental Limits and Practical  Attacks](https://arxiv.org//abs/2310.00076)

	Mehrdad Saberi, Vinu Sankar Sadasivan, Keivan Rezaei, Aounon Kumar, Atoosa Chegini, Wenxiao Wang, Soheil Feizi


+ [ Human-Producible Adversarial Examples](https://arxiv.org//abs/2310.00438)

	David Khachaturov, Yue Gao, Ilia Shumailov, Robert Mullins, Ross Anderson, Kassem Fawaz


+ [ Black-box Attacks on Image Activity Prediction and its Natural Language  Explanations](https://arxiv.org//abs/2310.00503)

	Alina Elena Baia, Valentina Poggioni, Andrea Cavallaro


+ [ GhostEncoder: Stealthy Backdoor Attacks with Dynamic Triggers to  Pre-trained Encoders in Self-supervised Learning](https://arxiv.org//abs/2310.00626)

	Qiannan Wang, Changchun Yin, Zhe Liu, Liming Fang, Run Wang, Chenhao Lin


+ [ Counterfactual Image Generation for adversarially robust and  interpretable Classifiers](https://arxiv.org//abs/2310.00761)

	Rafael Bischof, Florian Scheidegger, Michael A. Kraus, A. Cristiano I. Malossi


+ [ Practical Membership Inference Attacks Against Large-Scale Multi-Modal  Models: A Pilot Study](https://arxiv.org//abs/2310.00108)

	Myeongseob Ko, Ming Jin, Chenguang Wang, Ruoxi Jia


+ [ Understanding Adversarial Transferability in Federated Learning](https://arxiv.org//abs/2310.00616)

	Yijiang Li, Ying Gao, Haohan Wang


+ [ On the Onset of Robust Overfitting in Adversarial Training](https://arxiv.org//abs/2310.00607)

	Chaojian Yu, Xiaolong Shi, Jun Yu, Bo Han, Tongliang Liu


+ [ Balancing Efficiency vs. Effectiveness and Providing Missing Label  Robustness in Multi-Label Stream Classification](https://arxiv.org//abs/2310.00665)

	Sepehr Bakhshi, Fazli Can


+ [ Adversarial Explainability: Utilizing Explainable Machine Learning in  Bypassing IoT Botnet Detection Systems](https://arxiv.org//abs/2310.00070)

	Mohammed M. Alani, Atefeh Mashatan, Ali Miri


+ [ Source Inference Attacks: Beyond Membership Inference Attacks in  Federated Learning](https://arxiv.org//abs/2310.00222)

	Hongsheng Hu, Xuyun Zhang, Zoran Salcic, Lichao Sun, Kim-Kwang Raymond Choo, Gillian Dobbie


## 2023-10-02
+ [ AIR: Threats of Adversarial Attacks on Deep Learning-Based Information  Recovery](https://arxiv.org//abs/2309.16706)

	Jinyin Chen, Jie Ge, Shilian Zheng, Linhui Ye, Haibin Zheng, Weiguo Shen, Keqiang Yue, Xiaoniu Yang


+ [ General Lipschitz: Certified Robustness Against Resolvable Semantic  Transformations via Transformation-Dependent Randomized Smoothing](https://arxiv.org//abs/2309.16710)

	Dmitrii Korzh, Mikhail Pautov, Olga Tsymboi, Ivan Oseledets


+ [ Investigating Human-Identifiable Features Hidden in Adversarial  Perturbations](https://arxiv.org//abs/2309.16878)

	Dennis Y. Menn, Tzu-hsun Feng, Sriram Vishwanath, Hung-yi Lee


+ [ Medical Foundation Models are Susceptible to Targeted Misinformation  Attacks](https://arxiv.org//abs/2309.17007)

	Tianyu Han, Sven Nebelung, Firas Khader, Tianci Wang, Gustav Mueller-Franzes, Christiane Kuhl, Sebastian Försch, Jens Kleesiek, Christoph Haarburger, Keno K. Bressem, Jakob Nikolas Kather, Daniel Truhn



+ [ Adversarial Machine Learning in Latent Representations of Neural  Networks](https://arxiv.org//abs/2309.17401)

	Milin Zhang, Mohammad Abdi, Francesco Restuccia


+ [ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending  Against Extraction Attacks](https://arxiv.org//abs/2309.17410)

	Vaidehi Patil, Peter Hase, Mohit Bansal


+ [ LatticeGen: A Cooperative Framework which Hides Generated Text in a  Lattice for Privacy-Aware Generation on Cloud](https://arxiv.org//abs/2309.17157)

	Mengke Zhang, Tianxing He, Tianle Wang, Fatemehsadat Mireshghallah, Binyi Chen, Hao Wang, Yulia Tsvetkov


+ [ Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty  and Smoothness](https://arxiv.org//abs/2309.16973)

	Xiaoyu Wen, Xudong Yu, Rui Yang, Chenjia Bai, Zhen Wang


+ [ Efficient Biologically Plausible Adversarial Training](https://arxiv.org//abs/2309.17348)

	Matilde Tristany Farinha, Thomas Ortner, Giorgia Dellaferrera, Benjamin Grewe, Angeliki Pantazi


+ [ Adversarial Imitation Learning from Visual Observations using Latent  Information](https://arxiv.org//abs/2309.17371)

	Vittorio Giammarino, James Queeney, Ioannis Ch. Paschalidis


## 2023-10-01
+ [ Towards Efficient and Trustworthy AI Through  Hardware-Algorithm-Communication Co-Design](https://arxiv.org//abs/2309.15942)

	Bipin Rajendran, Osvaldo Simeone, Bashir M. Al-Hashimi


+ [ VDC: Versatile Data Cleanser for Detecting Dirty Samples via  Visual-Linguistic Inconsistency](https://arxiv.org//abs/2309.16211)

	Zihao Zhu, Mingda Zhang, Shaokui Wei, Bingzhe Wu, Baoyuan Wu


## 2023-09-30


+ [ Recent Advances of Differential Privacy in Centralized Deep Learning: A  Systematic Survey](https://arxiv.org//abs/2309.16398)

	Lea Demelius, Roman Kern, Andreas Trügler


+ [ Robust Offline Reinforcement Learning -- Certify the Confidence Interval](https://arxiv.org//abs/2309.16631)

	Jiarui Yao, Simon Shaolei Du


## 2023-09-29
+ [ Adversarial Examples Might be Avoidable: The Role of Data Concentration  in Adversarial Robustness](https://arxiv.org//abs/2309.16096)

	Ambar Pal, Jeremias Sulam, René Vidal


+ [ Parameter-Saving Adversarial Training: Reinforcing Multi-Perturbation  Robustness via Hypernetworks](https://arxiv.org//abs/2309.16207)

	Huihui Gong, Minjing Dong, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu


+ [ On the Trade-offs between Adversarial Robustness and Actionable  Explanations](https://arxiv.org//abs/2309.16452)

	Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju


+ [ Resisting Backdoor Attacks in Federated Learning via Bidirectional  Elections and Individual Perspective](https://arxiv.org//abs/2309.16456)

	Zhen Qin, Feiyi Chen, Chen Zhi, Xueqiang Yan, Shuiguang Deng


+ [ Towards Poisoning Fair Representations](https://arxiv.org//abs/2309.16487)

	Tianci Liu, Haoyu Wang, Feijie Wu, Hengtong Zhang, Pan Li, Lu Su, Jing Gao


+ [ Compilation as a Defense: Enhancing DL Model Attack Robustness via  Tensor Optimization](https://arxiv.org//abs/2309.16577)

	Stefan Trawicki, William Hackett, Lewis Birch, Neeraj Suri, Peter Garraghan


+ [ Cyber Sentinel: Exploring Conversational Agents in Streamlining Security  Tasks with GPT-4](https://arxiv.org//abs/2309.16422)

	Mehrdad Kaheh, Danial Khosh Kholgh, Panos Kostakos

## 2023-09-28

+ [ How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking  Unrelated Questions](https://arxiv.org//abs/2309.15840)

	Lorenzo Pacchiardi, Alex J. Chan, Sören Mindermann, Ilan Moscovitz, Alexa Y. Pan, Yarin Gal, Owain Evans, Jan Brauner


+ [ The Robust Semantic Segmentation UNCV2023 Challenge Results](https://arxiv.org//abs/2309.15478)

	Xuanlong Yu, Yi Zuo, Zitao Wang, Xiaowen Zhang, Jiaxuan Zhao, Yuting Yang, Licheng Jiao, Rui Peng, Xinyi Wang, Junpei Zhang, Kexin Zhang, Fang Liu, Roberto Alcover-Couso, Juan C. SanMiguel, Marcos Escudero-Viñolo, Hanlin Tian, Kenta Matsui, Tianhao Wang, Fahmy Adan, Zhitong Gao, Xuming He, Quentin Bouniot, Hossein Moghaddam, Shyam Nandan Rai, Fabio Cermelli, Carlo Masone, Andrea Pilzer, Elisa Ricci, Andrei Bursuc, Arno Solin, Martin Trapp, Rui Li, Angela Yao, Wenlong Chen, Ivor Simpson, Neill D. F. Campbell, Gianni Franchi


+ [ A Unified View of Differentially Private Deep Generative Modeling](https://arxiv.org//abs/2309.15696)

	Dingfan Chen, Raouf Kerkouche, Mario Fritz


+ [ On Computational Entanglement and Its Interpretation in Adversarial  Machine Learning](https://arxiv.org//abs/2309.15669)

	YenLung Lai, Xingbo Dong, Zhe Jin


+ [ Automatic Feature Fairness in Recommendation via Adversaries](https://arxiv.org//abs/2309.15418)

	Hengchang Hu, Yiming Cao, Zhankui He, Samson Tan, Min-Yen Kan


## 2023-09-27
+ [ Bias Assessment and Mitigation in LLM-based Code Generation](https://arxiv.org//abs/2309.14345)

	Dong Huang, Qingwen Bu, Jie Zhang, Xiaofei Xie, Junjie Chen, Heming Cui


+ [ Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM](https://arxiv.org//abs/2309.14348)

	Bochuan Cao, Yuanpu Cao, Lu Lin, Jinghui Chen


+ [ Survey of Social Bias in Vision-Language Models](https://arxiv.org//abs/2309.14381)

	Nayeon Lee, Yejin Bang, Holy Lovenia, Samuel Cahyawijaya, Wenliang Dai, Pascale Fung


+ [ XGV-BERT: Leveraging Contextualized Language Model and Graph Neural  Network for Efficient Software Vulnerability Detection](https://arxiv.org//abs/2309.14677)

	Vu Le Anh Quan, Chau Thuan Phat, Kiet Van Nguyen, Phan The Duy, Van-Hau Pham


+ [ DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature  Space](https://arxiv.org//abs/2309.14585)

	Liu Jun, Zhou Jiantao, Zeng Jiandian, Jinyu Tian


+ [ Structure Invariant Transformation for better Adversarial  Transferability](https://arxiv.org//abs/2309.14700)

	Xiaosen Wang, Zeliang Zhang, Jianping Zhang


+ [ Frugal Satellite Image Change Detection with Deep-Net Inversion](https://arxiv.org//abs/2309.14781)

	Hichem Sahbi, Sebastien Deschamps


+ [ The Surveillance AI Pipeline](https://arxiv.org//abs/2309.15084)

	Pratyusha Ria Kalluri, William Agnew, Myra Cheng, Kentrell Owens, Luca Soldaini, Abeba Birhane


+ [ Unveiling Fairness Biases in Deep Learning-Based Brain MRI  Reconstruction](https://arxiv.org//abs/2309.14392)

	Yuning Du, Yuyang Xue, Rohan Dharmakumar, Sotirios A. Tsaftaris


+ [ LogGPT: Log Anomaly Detection via GPT](https://arxiv.org//abs/2309.14482)

	Xiao Han, Shuhan Yuan, Mohamed Trabelsi


+ [ Privacy-preserving and Privacy-attacking Approaches for Speech and Audio  -- A Survey](https://arxiv.org//abs/2309.15087)

	Yuchen Liu, Apu Kapadia, Donald Williamson

## 2023-09-26
+ [ Investigating Efficient Deep Learning Architectures For Side-Channel  Attacks on AES](https://arxiv.org//abs/2309.13170)

	Yohaï-Eliel Berreby, Laurent Sauvage


+ [ Towards Green AI in Fine-tuning Large Language Models via Adaptive  Backpropagation](https://arxiv.org//abs/2309.13192)

	Kai Huang, Hanyun Yin, Heng Huang, Wei Gao


+ [ Defending Pre-trained Language Models as Few-shot Learners against  Backdoor Attacks](https://arxiv.org//abs/2309.13256)

	Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, Ting Wang


+ [ LLMs as Counterfactual Explanation Modules: Can ChatGPT Explain  Black-box Text Classifiers?](https://arxiv.org//abs/2309.13340)

	Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, Huan Liu


+ [ Seeing Is Not Always Believing: Invisible Collision Attack and Defence  on Pre-Trained Models](https://arxiv.org//abs/2309.13579)

	Minghang Deng, Zhong Zhang, Junming Shao


+ [ PRIS: Practical robust invertible network for image steganography](https://arxiv.org//abs/2309.13620)

	Hang Yang, Yitian Xu, Xuhua Liu, Xiaodong Ma


+ [ GHN-QAT: Training Graph Hypernetworks to Predict Quantization-Robust  Parameters of Unseen Limited Precision Neural Networks](https://arxiv.org//abs/2309.13773)

	Stone Yun, Alexander Wong


+ [ Can LLM-Generated Misinformation Be Detected?](https://arxiv.org//abs/2309.13788)

	Canyu Chen, Kai Shu


+ [ RBFormer: Improve Adversarial Robustness of Transformer by Robust Bias](https://arxiv.org//abs/2309.13245)

	Hao Cheng, Jinhao Duan, Hui Li, Lyutianyang Zhang, Jiahang Cao, Ping Wang, Jize Zhang, Kaidi Xu, Renjing Xu


+ [ DFRD: Data-Free Robustness Distillation for Heterogeneous Federated  Learning](https://arxiv.org//abs/2309.13546)

	Kangyang Luo, Shuai Wang, Yexuan Fu, Xiang Li, Yunshi Lan, Ming Gao


+ [ Vulnerabilities in Video Quality Assessment Models: The Challenge of  Adversarial Attacks](https://arxiv.org//abs/2309.13609)

	Ao-Xiang Zhang, Yu Ran, Weixuan Tang, Yuan-Gen Wang


+ [ Video Adverse-Weather-Component Suppression Network via Weather  Messenger and Adversarial Backpropagation](https://arxiv.org//abs/2309.13700)

	Yijun Yang, Angelica I. Aviles-Rivero, Huazhu Fu, Ye Liu, Weiming Wang, Lei Zhu


+ [ Combining Two Adversarial Attacks Against Person Re-Identification  Systems](https://arxiv.org//abs/2309.13763)

	Eduardo de O. Andrade, Igor Garcia Ballhausen Sampaio, Joris Guérin, José Viterbo


+ [ Adversarial Attacks on Video Object Segmentation with Hard Region  Discovery](https://arxiv.org//abs/2309.13857)

	Ping Li, Yu Zhang, Li Yuan, Jian Zhao, Xianghua Xu, Xiaoqin Zhang


+ [ SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via  Substitution](https://arxiv.org//abs/2309.14122)

	Zhongjie Ba, Jieming Zhong, Jiachen Lei, Peng Cheng, Qinglong Wang, Zhan Qin, Zhibo Wang, Kui Ren



+ [ Spatial-frequency channels, shape bias, and adversarial robustness](https://arxiv.org//abs/2309.13190)

	Ajay Subramanian, Elena Sizikova, Najib J. Majaj, Denis G. Pelli


+ [ Beyond Fairness: Age-Harmless Parkinson's Detection via Voice](https://arxiv.org//abs/2309.13292)

	Yicheng Wang, Xiaotian Han, Leisheng Yu, Na Zou


+ [ Improving Robustness of Deep Convolutional Neural Networks via  Multiresolution Learning](https://arxiv.org//abs/2309.13752)

	Hongyan Zhou, Yao Liang


+ [ Invisible Watermarking for Audio Generation Diffusion Models](https://arxiv.org//abs/2309.13166)

	Xirong Cao, Xiang Li, Divyesh Jadav, Yanzhao Wu, Zhehui Chen, Chen Zeng, Wenqi Wei


+ [ On the Effectiveness of Adversarial Samples against Ensemble  Learning-based Windows PE Malware Detectors](https://arxiv.org//abs/2309.13841)

	Trong-Nghia To, Danh Le Kim, Do Thi Thu Hien, Nghi Hoang Khoa, Hien Do Hoang, Phan The Duy, Van-Hau Pham


## 2023-09-25
+ [ Provably Robust and Plausible Counterfactual Explanations for Neural  Networks via Robust Optimisation](https://arxiv.org//abs/2309.12545)

	Junqi Jiang, Jianglin Lan, Francesco Leofante, Antonio Rago, Francesca Toni


+ [ HANS, are you clever? Clever Hans Effect Analysis of Neural Systems](https://arxiv.org//abs/2309.12481)

	Leonardo Ranaldi, Fabio Massimo Zanzotto


+ [ Privacy Assessment on Reconstructed Images: Are Existing Evaluation  Metrics Faithful to Human Perception?](https://arxiv.org//abs/2309.13038)

	Xiaoxiao Sun, Nidham Gazagnadou, Vivek Sharma, Lingjuan Lyu, Hongdong Li, Liang Zheng


+ [ Improving Machine Learning Robustness via Adversarial Training](https://arxiv.org//abs/2309.12593)

	Long Dang, Thushari Hapuarachchi, Kaiqi Xiong, Jing Lin


+ [ On Data Fabrication in Collaborative Vehicular Perception: Attacks and  Countermeasures](https://arxiv.org//abs/2309.12955)

	Qingzhao Zhang, Shuowei Jin, Jiachen Sun, Xumiao Zhang, Ruiyang Zhu, Qi Alfred Chen, Z. Morley Mao


+ [ Robotic Handling of Compliant Food Objects by Robust Learning from  Demonstration](https://arxiv.org//abs/2309.12856)

	Ekrem Misimi, Alexander Olofsson, Aleksander Eilertsen, Elling Ruud Øye, John Reidar Mathiassen


## 2023-09-24
+ [ Distilling Adversarial Prompts from Safety Benchmarks: Report for the  Adversarial Nibbler Challenge](https://arxiv.org//abs/2309.11575)

	Manuel Brack, Patrick Schramowski, Kristian Kersting


+ [ The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"](https://arxiv.org//abs/2309.12288)

	Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans


## 2023-09-23
+ [ Bad Actor, Good Advisor: Exploring the Role of Large Language Models in  Fake News Detection](https://arxiv.org//abs/2309.12247)

	Beizhe Hu, Qiang Sheng, Juan Cao, Yuhui Shi, Yang Li, Danding Wang, Peng Qi


+ [ A Chinese Prompt Attack Dataset for LLMs with Evil Content](https://arxiv.org//abs/2309.11830)

	Chengyuan Liu, Fubang Zhao, Lizhi Qing, Yangyang Kang, Changlong Sun, Kun Kuang, Fei Wu


+ [ Vulnerability of 3D Face Recognition Systems to Morphing Attacks](https://arxiv.org//abs/2309.12118)

	Sanjeet Vardam, Luuk Spreeuwers


+ [ Towards Differential Privacy in Sequential Recommendation: A Noisy Graph  Neural Network Approach](https://arxiv.org//abs/2309.11515)

	Wentao Hu, Hui Fang


## 2023-09-22
+ [ CATS: Conditional Adversarial Trajectory Synthesis for  Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches](https://arxiv.org//abs/2309.11587)

	Jinmeng Rao, Song Gao, Sijia Zhu


+ [ How Robust is Google's Bard to Adversarial Image Attacks?](https://arxiv.org//abs/2309.11751)

	Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, Jun Zhu


+ [ Knowledge Sanitization of Large Language Models](https://arxiv.org//abs/2309.11852)

	Yoichi Ishibashi, Hidetoshi Shimodaira


+ [ On the Relationship between Skill Neurons and Robustness in Prompt  Tuning](https://arxiv.org//abs/2309.12263)

	Leon Ackermann, Xenia Ohmer


+ [ TextCLIP: Text-Guided Face Image Generation And Manipulation Without  Adversarial Training](https://arxiv.org//abs/2309.11923)

	Xiaozhou You, Jian Zhang


+ [ Dictionary Attack on IMU-based Gait Authentication](https://arxiv.org//abs/2309.11766)

	Rajesh Kumar, Can Isik, Chilukuri K. Mohan
  

+ [ Privacy-Preserving In-Context Learning with Differentially Private  Few-Shot Generation](https://arxiv.org//abs/2309.11765)

	Xinyu Tang, Richard Shin, Huseyin A. Inan, Andre Manoel, Fatemehsadat Mireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Robert Sim


+ [ MarkNerf:Watermarking for Neural Radiance Field](https://arxiv.org//abs/2309.11747)

	Lifeng Chen, Jia Liu, Yan Ke, Wenquan Sun, Weina Dong, Xiaozhong Pan


+ [ DeepTheft: Stealing DNN Model Architectures through Power Side Channel](https://arxiv.org//abs/2309.11894)

	Yansong Gao, Huming Qiu, Zhi Zhang, Binghui Wang, Hua Ma, Alsharif Abuadbba, Minhui Xue, Anmin Fu, Surya Nepal


## 2023-09-21
+ [ When to Trust AI: Advances and Challenges for Certification of Neural  Networks](https://arxiv.org//abs/2309.11196)

	Marta Kwiatkowska, Xiyue Zhang


+ [ C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for  Physics-based Characters](https://arxiv.org//abs/2309.11351)

	Zhiyang Dou, Xuelin Chen, Qingnan Fan, Taku Komura, Wenping Wang


+ [ What Learned Representations and Influence Functions Can Tell Us About  Adversarial Examples](https://arxiv.org//abs/2309.10916)

	Shakila Mahjabin Tonni, Mark Dras


+ [ PRAT: PRofiling Adversarial aTtacks](https://arxiv.org//abs/2309.11111)

	Rahul Ambati, Naveed Akhtar, Ajmal Mian, Yogesh Singh Rawat


+ [ It's Simplex! Disaggregating Measures to Improve Certified Robustness](https://arxiv.org//abs/2309.11005)

	Andrew C. Cullen, Paul Montague, Shijie Liu, Sarah M. Erfani, Benjamin I.P. Rubinstein


+ [ Learning Patient Static Information from Time-series EHR and an Approach  for Safeguarding Privacy and Fairness](https://arxiv.org//abs/2309.11373)

	Wei Liao, Joel Voldman


+ [ Fed-LSAE: Thwarting Poisoning Attacks against Federated Cyber Threat  Detection System via Autoencoder-based Latent Space Inspection](https://arxiv.org//abs/2309.11053)

	Tran Duc Luong, Vuong Minh Tien, Nguyen Huu Quyen, Do Thi Thu Hien, Phan The Duy, Van-Hau Pham


## 2023-09-20
+ [ GPTFUZZER : Red Teaming Large Language Models with Auto-Generated  Jailbreak Prompts](https://arxiv.org//abs/2309.10253) 

  Jiahao Yu, Xingwei Lin, Xinyu Xing


+ [ Exploring the Dark Side of AI: Advanced Phishing Attack Design and  Deployment Using ChatGPT](https://arxiv.org//abs/2309.10463) 

  Nils Begou, Jeremy Vinoy, Andrzej Duda, Maciej Korczynski


+ [ Transferable Adversarial Attack on Image Tampering Localization](https://arxiv.org//abs/2309.10243)

  Yuqi Wang, Gang Cao, Zijie Lou, Haochen Zhu


+ [ RECALL+: Adversarial Web-based Replay for Continual Learning in Semantic  Segmentation](https://arxiv.org//abs/2309.10479)

  Chang Liu, Giulia Rizzoli, Francesco Barbato, Umberto Michieli, Yi Niu, Pietro Zanuttigh


+ [ Realistic Website Fingerprinting By Augmenting Network Trace](https://arxiv.org//abs/2309.10147)

  Alireza Bahramali, Ardavan Bozorgi, Amir Houmansadr


+ [ Love or Hate? Share or Split? Privacy-Preserving Training Using Split  Learning and Homomorphic Encryption](https://arxiv.org//abs/2309.10517) 

  Tanveer Khan, Khoa Nguyen, Antonis Michalas, Alexandros Bakas


+ [ Disentangled Information Bottleneck guided Privacy-Protective JSCC for  Image Transmission](https://arxiv.org//abs/2309.10263)

  Lunan Sun, Yang Yang, Mingzhe Chen, Caili Guo


+ [ SPFL: A Self-purified Federated Learning Method Against Poisoning  Attacks](https://arxiv.org//abs/2309.10607) 

  Zizhen Liu, Weiyang He, Chip-Hong Chang, Jing Ye, Huawei Li, Xiaowei Li


## 2023-09-19
+ [ Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents](https://arxiv.org//abs/2309.09919)

  Ziyi Yang, Shreyas S. Raman, Ankit Shah, Stefanie Tellex


+ [ Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract  Code Using Vulnerability-constrained Decoding](https://arxiv.org//abs/2309.09826)

  André Storhaug, Jingyue Li, Tianyuan Hu


+ [ Bias of AI-Generated Content: An Examination of News Produced by Large  Language Models](https://arxiv.org//abs/2309.09825)

  Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe Zhang, Ming Zhao, Xiaohang Zhao


+ [ Reducing Adversarial Training Cost with Gradient Approximation](https://arxiv.org//abs/2309.09464)

  Huihui Gong, Shuo Yang, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu


+ [ Stealthy Physical Masked Face Recognition Attack via Adversarial Style  Optimization](https://arxiv.org//abs/2309.09480)

  Huihui Gong, Minjing Dong, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu


+ [ Evaluating Adversarial Robustness with Expected Viable Performance](https://arxiv.org//abs/2309.09928)

  Ryan McCoppin, Colin Dawson, Sean M. Kennedy, Leslie M. Blaha

## 2023-09-17

+ [ Convex Latent-Optimized Adversarial Regularizers for Imaging Inverse  Problems](https://arxiv.org//abs/2309.09250)

  Huayu Wang, Chen Luo, Taofeng Xie, Qiyu Jin, Guoqing Chen, Zhuo-Xu Cui, Dong Liang

## 2023-09-16


+ [ Robust Backdoor Attacks on Object Detection in Real World](https://arxiv.org//abs/2309.08953)

  Yaguan Qian, Boyuan Ji, Shuke He, Shenhui Huang, Xiang Ling, Bin Wang, Wei Wang


+ [ Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and  Nationality Bias in Generative Models](https://arxiv.org//abs/2309.08902)

  Mahammed Kamruzzaman, Md. Minul Islam Shovon, Gene Louis Kim
 

+ [ Context-aware Adversarial Attack on Named Entity Recognition](https://arxiv.org//abs/2309.08999)

  Shuguang Chen, Leonardo Neves, Thamar Solorio
 
## 2023-09-15

+ [ Adversarial Attacks on Tables with Entity Swap](https://arxiv.org//abs/2309.08650)

  Aneta Koleva, Martin Ringsquandl, Volker Tresp


+ [ A More Secure Split: Enhancing the Security of Privacy-Preserving Split  Learning](https://arxiv.org//abs/2309.08697)

  Tanveer Khan, Khoa Nguyen, Antonis Michalas

  
+ [ Detecting Unknown Attacks in IoT Environments: An Open Set Classifier  for Enhanced Network Intrusion Detection](https://arxiv.org//abs/2309.07461)

  Yasir Ali Farrukh, Syed Wali, Irfan Khan, Nathaniel D. Bastian


+ [ Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated  Text](https://arxiv.org//abs/2309.07689)

  Mahdi Dhaini, Wessel Poelman, Ege Erdogan


+ [ Keep your Identity Small: Privacy-preserving Client-side Fingerprinting](https://arxiv.org//abs/2309.07563)

  Alberto Fernandez-de-Retana, Igor Santos-Grueiro


+ [ Fake News Detectors are Biased against Texts Generated by Large Language  Models](https://arxiv.org//abs/2309.08674)

  Jinyan Su, Terry Yue Zhuo, Jonibek Mansurov, Di Wang, Preslav Nakov
